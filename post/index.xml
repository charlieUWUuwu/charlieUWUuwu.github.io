<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Charlie&#39;s Blog</title>
        <link>https://charlieUWUuwu.github.io/post/</link>
        <description>Recent content in Posts on Charlie&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Charlie</copyright>
        <lastBuildDate>Fri, 26 Apr 2024 00:06:37 +0800</lastBuildDate><atom:link href="https://charlieUWUuwu.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Pixel2style2pixel</title>
        <link>https://charlieUWUuwu.github.io/p/pixel2style2pixel/</link>
        <pubDate>Fri, 26 Apr 2024 00:06:37 +0800</pubDate>
        
        <guid>https://charlieUWUuwu.github.io/p/pixel2style2pixel/</guid>
        <description>&lt;p&gt;先介紹 PGGAN &amp;gt; StyleGAN &amp;gt; pSp&lt;/p&gt;
&lt;script src= &#39;/js/pdf-js/build/pdf.js&#39;&gt;&lt;/script&gt;

&lt;style&gt;
  #embed-pdf-container {
    position: relative;
    width: 100%;
    height: auto;
    min-height: 20vh;
     
  }
  
  .pdf-canvas {
    border: 1px solid black;
    direction: ltr;
    width: 100%;
    height: auto;
    display: none;
  }
  
  #the-canvas {
    border: 1px solid black;
    direction: ltr;
    width: 100%;
    height: auto;
    display: none;
  }
  
  
  .pdf-loadingWrapper {
    display: none;
    justify-content: center;
    align-items: center;
    width: 100%;
    height: 350px;
  }
  
  .pdf-loading {
    display: inline-block;
    width: 50px;
    height: 50px;
    border: 3px solid #d2d0d0;;
    border-radius: 50%;
    border-top-color: #383838;
    animation: spin 1s ease-in-out infinite;
    -webkit-animation: spin 1s ease-in-out infinite;
  }
  
  
  
  
  
  #overlayText {
    word-wrap: break-word;
    display: grid;
    justify-content: end;
  }
  
  #overlayText a {
    position: relative;
    top: 10px;
    right: 4px;
    color: #000;
    margin: auto;
    background-color: #eeeeee;
    padding: 0.3em 1em;
    border: solid 2px;
    border-radius: 12px;
    border-color: #00000030;
    text-decoration: none;
  }
  
  #overlayText svg {
    height: clamp(1em, 2vw, 1.4em);
    width:  clamp(1em, 2vw, 1.4em);
  }
  
  
  
  @keyframes spin {
    to { -webkit-transform: rotate(360deg); }
  }
  @-webkit-keyframes spin {
    to { -webkit-transform: rotate(360deg); }
  }
  &lt;/style&gt;&lt;div class=&#34;embed-pdf-container&#34; id=&#34;embed-pdf-container-1cec89bc&#34;&gt;
    &lt;div class=&#34;pdf-loadingWrapper&#34; id=&#34;pdf-loadingWrapper-1cec89bc&#34;&gt;
        &lt;div class=&#34;pdf-loading&#34; id=&#34;pdf-loading-1cec89bc&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div id=&#34;overlayText&#34;&gt;
      &lt;a href=&#34;https://charlieUWUuwu.github.io/file/styleGAN.pdf&#34; aria-label=&#34;Download&#34; download&gt;
        &lt;svg aria-hidden=&#34;true&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 18 18&#34;&gt;
            &lt;path d=&#34;M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z&#34; /&gt;
        &lt;/svg&gt;
      &lt;/a&gt;
    &lt;/div&gt;
    &lt;canvas class=&#34;pdf-canvas&#34; id=&#34;pdf-canvas-1cec89bc&#34;&gt;&lt;/canvas&gt;
&lt;/div&gt;

&lt;div class=&#34;pdf-paginator&#34; id=&#34;pdf-paginator-1cec89bc&#34;&gt;
    &lt;button id=&#34;pdf-prev-1cec89bc&#34;&gt;Previous&lt;/button&gt;
    &lt;button id=&#34;pdf-next-1cec89bc&#34;&gt;Next&lt;/button&gt; &amp;nbsp; &amp;nbsp;
    &lt;span&gt;
      &lt;span class=&#34;pdf-pagenum&#34; id=&#34;pdf-pagenum-1cec89bc&#34;&gt;&lt;/span&gt; / &lt;span class=&#34;pdf-pagecount&#34; id=&#34;pdf-pagecount-1cec89bc&#34;&gt;&lt;/span&gt;
    &lt;/span&gt;
    &lt;a class=&#34;pdf-source&#34; id=&#34;pdf-source-1cec89bc&#34; href=&#34;https://charlieUWUuwu.github.io/file/styleGAN.pdf&#34;&gt;[pdf]&lt;/a&gt;
&lt;/div&gt;

&lt;noscript&gt;
View the PDF file &lt;a class=&#34;pdf-source&#34; id=&#34;pdf-source-noscript-1cec89bc&#34; href=&#34;https://charlieUWUuwu.github.io/file/styleGAN.pdf&#34;&gt;here&lt;/a&gt;.
&lt;/noscript&gt;

&lt;script type=&#34;text/javascript&#34;&gt;
    (function(){
    var url = &#39;\/file\/styleGAN.pdf&#39;;

    var hidePaginator = &#34;&#34; === &#34;true&#34;;
    var hideLoader = &#34;&#34; === &#34;true&#34;;
    var selectedPageNum = parseInt(&#34;&#34;) || 1;

    
    var pdfjsLib = window[&#39;pdfjs-dist/build/pdf&#39;];

    
    if (pdfjsLib.GlobalWorkerOptions.workerSrc == &#39;&#39;)
      pdfjsLib.GlobalWorkerOptions.workerSrc = &#34;https:\/\/charlieUWUuwu.github.io\/&#34; + &#39;js/pdf-js/build/pdf.worker.js&#39;;

    
    var pdfDoc = null,
        pageNum = selectedPageNum,
        pageRendering = false,
        pageNumPending = null,
        scale = 3,
        canvas = document.getElementById(&#39;pdf-canvas-1cec89bc&#39;),
        ctx = canvas.getContext(&#39;2d&#39;),
        paginator = document.getElementById(&#34;pdf-paginator-1cec89bc&#34;),
        loadingWrapper = document.getElementById(&#39;pdf-loadingWrapper-1cec89bc&#39;);


    
    showPaginator();
    showLoader();

    

    function renderPage(num) {
      pageRendering = true;
      
      pdfDoc.getPage(num).then(function(page) {
        var viewport = page.getViewport({scale: scale});
        canvas.height = viewport.height;
        canvas.width = viewport.width;

        
        var renderContext = {
          canvasContext: ctx,
          viewport: viewport
        };
        var renderTask = page.render(renderContext);

        
        renderTask.promise.then(function() {
          pageRendering = false;
          showContent();

          if (pageNumPending !== null) {
            
            renderPage(pageNumPending);
            pageNumPending = null;
          }
        });
      });

      
      document.getElementById(&#39;pdf-pagenum-1cec89bc&#39;).textContent = num;
    }

    

    function showContent() {
      loadingWrapper.style.display = &#39;none&#39;;
      canvas.style.display = &#39;block&#39;;
    }

    

    function showLoader() {
      if(hideLoader) return
      loadingWrapper.style.display = &#39;flex&#39;;
      canvas.style.display = &#39;none&#39;;
    }

    

    function showPaginator() {
      if(hidePaginator) return
      paginator.style.display = &#39;block&#39;;
    }

    

    function queueRenderPage(num) {
      if (pageRendering) {
        pageNumPending = num;
      } else {
        renderPage(num);
      }
    }

    

    function onPrevPage() {
      if (pageNum &lt;= 1) {
        return;
      }
      pageNum--;
      queueRenderPage(pageNum);
    }
    document.getElementById(&#39;pdf-prev-1cec89bc&#39;).addEventListener(&#39;click&#39;, onPrevPage);

    

    function onNextPage() {
      if (pageNum &gt;= pdfDoc.numPages) {
        return;
      }
      pageNum++;
      queueRenderPage(pageNum);
    }
    document.getElementById(&#39;pdf-next-1cec89bc&#39;).addEventListener(&#39;click&#39;, onNextPage);

    

    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
      pdfDoc = pdfDoc_;
      var numPages = pdfDoc.numPages;
      document.getElementById(&#39;pdf-pagecount-1cec89bc&#39;).textContent = numPages;

      
      if(pageNum &gt; numPages) {
        pageNum = numPages
      }

      
      renderPage(pageNum);
    });
    })();
&lt;/script&gt;

&lt;h1 id=&#34;pix2style2pixpsp-論文閱讀&#34;&gt;
    &lt;a href=&#34;#pix2style2pixpsp-%e8%ab%96%e6%96%87%e9%96%b1%e8%ae%80&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    pix2style2pix(pSp) 論文閱讀
&lt;/h1&gt;&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2008.00951&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Abstract
&lt;/h2&gt;&lt;p&gt;先前 styleGAN 已經可以生成高品質的人臉，且可以控制圖像特定特徵(完成特徵解纏)，但主要是針對生成器架構做改進，因此還不能進行 img2img 任務。而 pSp 進行了以下改善 :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出&lt;strong&gt;通用框架 pixel2style2pixel(pSp)&lt;/strong&gt;，實現 image-to-image
&lt;ul&gt;
&lt;li&gt;新的 encoder + 預訓練 styleGAN generator(decoder)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;提出新的 encoder 架構
&lt;ul&gt;
&lt;li&gt;基於特徵金字塔網路 (Feature Pyramid Network)&lt;/li&gt;
&lt;li&gt;能夠產生 style vector 並輸入到預訓練的 StyleGAN 生成器，形成 extended $\textit{W+}$ latent space&lt;/li&gt;
&lt;li&gt;不需進行耗時最佳化&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不需要使用成對訓練資料&lt;/li&gt;
&lt;li&gt;透過對 styleGAN 做 resample，進行多模態合成
&lt;ul&gt;
&lt;li&gt;訓練 encoder 時引入 ID loss，避免影像重建後變了一個人(更精準的控制細節)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;
    &lt;a href=&#34;#introduction&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Introduction
&lt;/h2&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/1_intro.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/1_intro.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;先前 styleGAN 是「invert first, edit later」(先反轉到latent space，再生成影像)，但將真實影像反轉為 512 維向量 w ∈ W 不能做準確的重建。而 pSp 將真實影像編碼到 W+，也就是直接將輸入影像編碼為對應的輸出潛在影像，從而允許處理不屬於 StyleGAN 域的輸入。&lt;/p&gt;
&lt;p&gt;W+ latent space 由 18 個不同的 512 維 w 向量串聯(18x512x1)，且每一個向量會送給 styleGAN 每一個輸入層，但單一影像用此方法優化就會需要好幾分鐘。因此將真實影像快速且準確地反演為 W+ 仍然是一個挑戰。&lt;/p&gt;
&lt;p&gt;遵循 pix2pix 精神，定義能夠解決所有任務的通用架構。pix2style2pix 表示先將圖像用 encoder 編碼成 style vector，然後再用 decoer 轉回圖像。&lt;/p&gt;
&lt;p&gt;不需對手鑑別器 (adversary discriminator)，而直接使用預訓練 styleGAN，因此有以下優勢 :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成器僅受 style 控制，沒有直接的空間輸入&lt;/li&gt;
&lt;li&gt;intermediate style 適合模糊任務的【多模態合成】 (例如 sketch2img、影像高清化等，可以做髮色改變之類的)。這是因為可以對生成的 style 重新採樣來創建輸出圖像的變體。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-psp-framework&#34;&gt;
    &lt;a href=&#34;#the-psp-framework&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    The pSp Framework
&lt;/h2&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/2_framework.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/2_framework.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;使用一個強大的 encoder，能將每個輸入影像與 latent domain 中的準確編碼相匹配。將圖像嵌入到 latent domain 有一些做法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(傳統) 從 encoder 最後一層獲得單一 512 維向量，直接 repeat 18 次成為 W+，從而一起學習所有 18 個風格向量。但難以表示原始影像的精細細節，導致重建品質受限。&lt;/li&gt;
&lt;li&gt;(pSp) 先前 styleGAN 的特徵依據分層結構的深淺，代表不同粒度的特徵。pSp 依此出發，使用特徵金字塔擴展 encoder 主幹，產生三個層級的特徵圖，從全連接層(map2style)提取 style(如上圖) 產生 latent code(512*1)。接著輸入到生成器中對應的地方以產生影像。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這邊將輸入定義為 :&lt;/p&gt;
&lt;p&gt;$pSp(x):=G(E(x)+\bar{w})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;x: 輸入影像&lt;/li&gt;
&lt;li&gt;E(·) 和 G(·): encoder 跟 StyleGAN 生成器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;公式中，encoder 旨在學習相對於 average style vector 的 latent code。&lt;/p&gt;
&lt;h3 id=&#34;loss-functions&#34;&gt;
    &lt;a href=&#34;#loss-functions&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Loss Functions
&lt;/h3&gt;&lt;p&gt;encoder 的訓練是透過多個 loss 的加權組合而成的。&lt;/p&gt;
&lt;p&gt;$\mathcal{L_2}(x)=|x-pSp(x)|_2$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pixel-wise L2 loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathcal{L}_{PIPS}(x)=|F(x)-F(pSp(x))|_2$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;為了學習感知相似性，利用 LPIPS 損失(已被證明可以更好地保持影像品質)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathcal{L}_{reg}(x)=|E(x)-\bar{w}|_2$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;鼓勵編碼器輸出更接近平均latent vector的latent styke vector，另外定義的正規化損失&lt;/li&gt;
&lt;li&gt;與 StyleGAN 中的截斷技巧類似，作者發現在encoder的訓練中添加這種正則化可以提高圖像品質，而不損害輸出的保真度，特別是在更模糊的任務中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathcal{L}_{ID}(x)=1-&amp;lt;R(x),\ R(pSp(x))&amp;gt;$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;處理臉部影像編碼的特定任務時的一個常見挑戰是保存輸入身分，因此採用專用的辨識損失來測量輸出影像與其來源影像之間的餘弦相似度&lt;/li&gt;
&lt;li&gt;R: 預訓練的 ArcFace 模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\mathcal{L}(x)=\lambda_1\mathcal{L}&lt;em&gt;2(x)+\lambda_2\mathcal{L}&lt;/em&gt;{PIPS}(x)+\lambda_3\mathcal{L}&lt;em&gt;{ID}(x)+\lambda_4\mathcal{L}&lt;/em&gt;{reg}(x)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最終結果。其中$\lambda_1$~$\lambda_4$是定義損失權重的常數。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-benefits-of-the-stylegan-domain&#34;&gt;
    &lt;a href=&#34;#the-benefits-of-the-stylegan-domain&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    The Benefits of The StyleGAN Domain
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/3_stylemixing.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/3_stylemixing.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;透過 style domain 做圖像轉換使 pSp 在全局而非local運行，因此不需要像 pix2pix 的對應。&lt;/p&gt;
&lt;p&gt;由於某些翻譯任務是不明確的，其中單一輸入影像可能對應於多個輸出，因此希望能夠對這些可能的輸出進行取樣。&lt;/p&gt;
&lt;p&gt;前人文獻提到特徵解纏(語義分離)是因為 layer-wise representation 帶來的，因為能獨立操作特徵，自然就會想要「&lt;strong&gt;多模態數據生成&lt;/strong&gt;」。&lt;/p&gt;
&lt;p&gt;論文做法如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;圖片經過 encoder 生成 latent code(18x512x1)，輸入給 style mixing 網路內的 1~7 層使用&lt;/li&gt;
&lt;li&gt;隨機採樣向量 $w ∈ R^{512}$ ，並複製 w 在 W+ 中產生相應的 latent code，輸入給style mixing 網路內的剩下層。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;可以加入參數來控制兩者的融合，在 pSp 中是在 coarse 跟 medium 層面上使用圖像經 encode 輸出的 style，fine 才使用隨機變量，以保證產生的是同一個人，但有不同的細節變化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;applications-and-experiments&#34;&gt;
    &lt;a href=&#34;#applications-and-experiments&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Applications and Experiments
&lt;/h2&gt;&lt;p&gt;在許多 img2img 的任務尚實驗，確定方法的有效性。&lt;/p&gt;
&lt;h3 id=&#34;stylegan-inversion&#34;&gt;
    &lt;a href=&#34;#stylegan-inversion&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    StyleGAN Inversion
&lt;/h3&gt;&lt;p&gt;在 latent domain 中找到 real image 的 leatent code。也就是要比較 encoder 轉換後的 latent code，能否有比較準確的生成效果。&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/4_stylegan_inversion.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/4_stylegan_inversion.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;
結果 :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ALAE 方法在 W 域中運行，無法準確地重建輸入影像。&lt;/li&gt;
&lt;li&gt;雖然 IDInvert 更好地保留了影像屬性，但它仍然無法準確保留輸入影像的身份和更精細的細節。相較之下，pSp 能夠保留身份，同時重建燈光、髮型和眼鏡等細節。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ IDInvert中，首先將要轉換的圖像編碼成潛在向量（latent code），這個潛在向量是StyleGAN預訓練模型的潛在空間中的一個點。然後，通過對生成的圖像進行直接優化，調整這個潛在向量，使生成的圖像盡可能地接近原始圖像。這樣做的目的是在不重新訓練整個StyleGAN模型的情況下，將圖像轉換到StyleGAN的潛在空間中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/5_stylegan_inversion2.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/5_stylegan_inversion2.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;
接著做 pSp 架構的消融實驗&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;W : 從 encoder 最後一層提取出在 512 維的 style vector(屬於 W latent domain)&lt;/li&gt;
&lt;li&gt;Naive W+ : 承上，接著使用具有附加層的 encoder，將 1x512 特徵向量轉為 18x512 的 W+ 向量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;結果 :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;W+ 擴展顯著改善了結果，但仍然無法保留更精細的細節。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/6_stylegan_inversion3.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/6_stylegan_inversion3.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;
有 ID loss 能夠帶來更好的生分特徵(重建後不會變一個人)&lt;/p&gt;
&lt;p&gt;接下來就是對不同影像生成任務情景做實驗觀察，詳見論文。&lt;/p&gt;
&lt;h3 id=&#34;face-frontalization&#34;&gt;
    &lt;a href=&#34;#face-frontalization&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Face Frontalization
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;由於需要非局部轉換並且缺乏配對訓練數據，臉部正面化對於影像到影像轉換框架來說是一項具有挑戰性的任務。
略&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;conditional-image-synthesis&#34;&gt;
    &lt;a href=&#34;#conditional-image-synthesis&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Conditional Image Synthesis
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;條件影像合成旨在產生基於某些輸入類型的逼真影像。在本節中，我們的 pSp 架構在兩個條件影像生成任務上進行了測試。
從草圖和語意分割圖產生高品質的人臉影像。
我們證明，只需進行最小的更改，我們的編碼器就可以成功利用 StyleGAN 的表達能力來產生高品質和多樣化的輸出。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;face-from-sketch&#34;&gt;
    &lt;a href=&#34;#face-from-sketch&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Face From Sketch
&lt;/h4&gt;&lt;p&gt;草圖到真實圖過往需要成對資料，然而 pSp 不需要。&lt;/p&gt;
&lt;p&gt;作者團隊建立了草圖影像資料集，有很多人臉手繪插圖&lt;/p&gt;
&lt;p&gt;作者觀察到 pSp 獲得更多樣化輸出的能力，這些輸出可以更好地保留更精細的細節（例如臉部毛髮）。&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;
    &lt;a href=&#34;#discussion&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Discussion
&lt;/h2&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/7_discussion.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/pixel2style2pixel/7_discussion.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;pSp 仍然有一些固有假設(inherent assumptions):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;產生圖像受限
&lt;ul&gt;
&lt;li&gt;因為使用預訓練 styleGAN，所以能夠生成的圖像僅限於 styleGAN 可以產生的圖像。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;目前仍難以保留輸入影像的更精細細節(見上圖)
&lt;ul&gt;
&lt;li&gt;pSp 的全局方法在不同任務上的通用性高，但還無法保留輸入影像的更精細細節（例如耳環或背景細節）&lt;/li&gt;
&lt;li&gt;這對於影像修復等任務很重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ 其他實驗見論文拉&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ref&#34;&gt;
    &lt;a href=&#34;#ref&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Ref
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/xjm850552586/article/details/123113080&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;pixel2style2pixel（pSp）实现解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Chat_streamer</title>
        <link>https://charlieUWUuwu.github.io/p/chat_streamer/</link>
        <pubDate>Wed, 10 Apr 2024 03:30:04 +0800</pubDate>
        
        <guid>https://charlieUWUuwu.github.io/p/chat_streamer/</guid>
        <description>&lt;h2 id=&#34;說明&#34;&gt;
    &lt;a href=&#34;#%e8%aa%aa%e6%98%8e&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    說明
&lt;/h2&gt;&lt;p&gt;就像 ChatGPT 那樣一個字一個字跑出來。這個方法是在 &lt;a class=&#34;link&#34; href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLaMA-Factory&lt;/a&gt; 看到的。
完整 code 可以在&lt;a class=&#34;link&#34; href=&#34;https://github.com/charlieUWUuwu/NTTU_meta_campus_chat/blob/develop/src/llmtuner/chat/streamer.py&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;這個專案&lt;/a&gt;裡面找&lt;/p&gt;
&lt;h2 id=&#34;用-huggingface-模型做流式輸出&#34;&gt;
    &lt;a href=&#34;#%e7%94%a8-huggingface-%e6%a8%a1%e5%9e%8b%e5%81%9a%e6%b5%81%e5%bc%8f%e8%bc%b8%e5%87%ba&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    用 HuggingFace 模型做流式輸出
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;HuggingFace 的 v4.30.1 加入了兩個流式輸出的接口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TextStreamer: 能在 stdout 中流式輸出结果&lt;/li&gt;
&lt;li&gt;TextIteratorStreamer：能在自定義 loop 中進行操作&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;textstreamer&#34;&gt;
    &lt;a href=&#34;#textstreamer&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    TextStreamer
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 來自 https://huggingface.co/docs/transformers/main/generation_strategies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;transformers&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextStreamer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_pretrained&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_pretrained&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;An increasing sequence: one,&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;return_tensors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextStreamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Despite returning the usual output, the streamer will also print the generated text to stdout.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_new_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;textiteratorstreamer&#34;&gt;
    &lt;a href=&#34;#textiteratorstreamer&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    TextIteratorStreamer
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 來自 https://huggingface.co/docs/transformers/main/generation_strategies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;transformers&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextIteratorStreamer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;threading&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Thread&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_pretrained&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_pretrained&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;An increasing sequence: one,&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;return_tensors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextIteratorStreamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tok&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;generation_kwargs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_new_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;thread&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Thread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generation_kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;thread&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;generated_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;generated_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;generated_text&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;用-chatglm-示範&#34;&gt;
    &lt;a href=&#34;#%e7%94%a8-chatglm-%e7%a4%ba%e7%af%84&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    用 ChatGLM 示範
&lt;/h3&gt;&lt;p&gt;參考 &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_44491772/article/details/131205174&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现&lt;/a&gt; 中的「ChatGLM流式回覆Demo」部分&lt;/p&gt;
&lt;h2 id=&#34;讓-openai-模型也能流式輸出&#34;&gt;
    &lt;a href=&#34;#%e8%ae%93-openai-%e6%a8%a1%e5%9e%8b%e4%b9%9f%e8%83%bd%e6%b5%81%e5%bc%8f%e8%bc%b8%e5%87%ba&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    讓 OpenAI 模型也能流式輸出
&lt;/h2&gt;&lt;p&gt;參考 &lt;a class=&#34;link&#34; href=&#34;https://github.com/huggingface/transformers/blob/main/src/transformers/generation/streamers.py#L24&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Huggingface 官方實現的 BaseStreamer&lt;/a&gt; 寫出來的。&lt;/p&gt;
&lt;h3 id=&#34;設定-openai-模型&#34;&gt;
    &lt;a href=&#34;#%e8%a8%ad%e5%ae%9a-openai-%e6%a8%a1%e5%9e%8b&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    設定 OpenAI 模型
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;streaming&lt;/code&gt;設置為&lt;code&gt;True&lt;/code&gt;，可以不用等一句話完全生完後才得到回覆&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StreamingStdOutCallbackHandler&lt;/code&gt; 是 Langchain 的一個 callback 處理器，會在語言模型模型生成新 token 時被觸發(&lt;code&gt;on_llm_new_token&lt;/code&gt;())，並透過&lt;code&gt;sys.stdout.write(token)&lt;/code&gt;與&lt;code&gt;sys.stdout.flush()&lt;/code&gt;來確保輸出即時可見。
&lt;ul&gt;
&lt;li&gt;說明可見&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/China_boy007/article/details/136445246&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;這個部落格&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain_openai&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ChatOpenAI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;langchain.callbacks.streaming_stdout&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StreamingStdOutCallbackHandler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ChatOpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;streaming&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;callbacks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;StreamingStdOutCallbackHandler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;temperature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;撰寫-chatgptstreamer&#34;&gt;
    &lt;a href=&#34;#%e6%92%b0%e5%af%ab-chatgptstreamer&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    撰寫 ChatgptStreamer
&lt;/h3&gt;&lt;p&gt;主要是透過 Python 的 Iterator 來實現的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;_on_finalized_text()&lt;/code&gt; : 將收到的 text 存到 queue，直到收到 stream 結束訊號(stream_end==True)，&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model_generate()&lt;/code&gt; : 接收語言模型並調用&lt;code&gt;.stream()&lt;/code&gt;來獲得流式輸出，最後後塞入&lt;code&gt;stop_signal&lt;/code&gt;表輸出結束。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;queue&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Queue&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;ChatgptStreamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_queue&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Queue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stop_signal&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;_on_finalized_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream_end&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_queue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;put&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream_end&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_queue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;put&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stop_signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model_generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;請你根據以下參考資料回答問題。&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;相關資料:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;問題:&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_on_finalized_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_on_finalized_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stop_signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__iter__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;21&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;23&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__next__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_queue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stop_signal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;26&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;StopIteration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;27&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;28&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;streamerthread-進行生成&#34;&gt;
    &lt;a href=&#34;#streamerthread-%e9%80%b2%e8%a1%8c%e7%94%9f%e6%88%90&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Streamer+Thread 進行生成
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nd&#34;&gt;@torch.inference_mode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;stream_chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Optional&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Optional&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_kwargs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Generator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;gen_kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_process_args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gen_kwargs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 使用 ChatOpenAI &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ChatgptStreamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;thread&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Thread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 使用 Huggingface 模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TextIteratorStreamer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timeout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;60.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;skip_prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;skip_special_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;gen_kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;streamer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;thread&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Thread&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gen_kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;21&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;thread&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;yield from&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;streamer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;使用&#34;&gt;
    &lt;a href=&#34;#%e4%bd%bf%e7%94%a8&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    使用
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;chatbot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Generator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tuple&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]],&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;chatbot&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;chatbot : &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chatbot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 從 VDB 獲取資料&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;docs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectordb_manager&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_results&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 回傳最相關的 5 筆相關資料&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;docs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;doc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;page_content&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;系統提示詞 : &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stream_chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;21&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;chatbot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;postprocess&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;ln&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;yield&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chatbot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;refs-&#34;&gt;
    &lt;a href=&#34;#refs-&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    refs :
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_44491772/article/details/131205174&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://huggingface.co/docs/transformers/main/generation_strategies&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;HuggingFace 官方文件&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>SQUID</title>
        <link>https://charlieUWUuwu.github.io/p/squid/</link>
        <pubDate>Wed, 10 Apr 2024 03:30:04 +0800</pubDate>
        
        <guid>https://charlieUWUuwu.github.io/p/squid/</guid>
        <description>&lt;h1 id=&#34;squid-deep-feature-in-painting-for-unsupervised-anomaly-detection&#34;&gt;
    &lt;a href=&#34;#squid-deep-feature-in-painting-for-unsupervised-anomaly-detection&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection
&lt;/h1&gt;&lt;p&gt;paper : &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.13495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2111.13495&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Abstract
&lt;/h1&gt;&lt;p&gt;射線照相成像協議專注於特定的身體區域，因此產生非常相似的影像並產生跨患者的重複解剖結構。&lt;/p&gt;
&lt;p&gt;為了利用這種結構化訊息，使用 Space-aware Memory Queues for In-painting and Detecting（SQUID）來修復和偵測射線照相影像中的異常。&lt;/p&gt;
&lt;p&gt;證明 SQUID 可以將根深蒂固的解剖結構分類為循環模式(recurrent patterns)。(它可以把固有的人體結構分類為反覆出現的 pattern)&lt;/p&gt;
&lt;p&gt;在推理中，它&lt;strong&gt;可以識別圖像中的異常（未見/修改的模式）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;作者創建了一個新的資料集（DigitAnatomy），綜合了胸部解剖學中的空間相關性和一致的形狀。&lt;/p&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;
    &lt;a href=&#34;#1-introduction&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    1. Introduction
&lt;/h1&gt;&lt;h2 id=&#34;放射成像-vs-一般影像&#34;&gt;
    &lt;a href=&#34;#%e6%94%be%e5%b0%84%e6%88%90%e5%83%8f-vs-%e4%b8%80%e8%88%ac%e5%bd%b1%e5%83%8f&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    放射成像 vs 一般影像
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;普通視覺任務有平移不變性的假設(貓出現在左或是右，還是貓)；但放射線成像(radiography imaging)，結構的相對位置和方向是識別的重要特徵。&lt;/li&gt;
&lt;li&gt;且因放射成像在不同患者或設備商評估方式幾乎相似，故一致且重複的解剖結構有助於分析許多關鍵問題，並應被視為放射線成像的顯著優勢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;放射成像的先驗知識&#34;&gt;
    &lt;a href=&#34;#%e6%94%be%e5%b0%84%e6%88%90%e5%83%8f%e7%9a%84%e5%85%88%e9%a9%97%e7%9f%a5%e8%ad%98&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    放射成像的先驗知識
&lt;/h2&gt;&lt;p&gt;有多項研究顯示添加位置特徵、修改目標函數以及約束相對於影像中地標的座標能增強深度網路效能。此偏論文希望能利用一致的解剖模式及其空間資訊，加強深度網路對放射線影像異常的檢測，而無需手動註釋。&lt;/p&gt;
&lt;h2 id=&#34;醫療異常檢測作用&#34;&gt;
    &lt;a href=&#34;#%e9%86%ab%e7%99%82%e7%95%b0%e5%b8%b8%e6%aa%a2%e6%b8%ac%e4%bd%9c%e7%94%a8&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    醫療異常檢測作用
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;80% 的臨床錯誤是由於放射科醫師漏掉異常造成&lt;/li&gt;
&lt;li&gt;異常檢測作用是向放射科醫生明確指出存在可疑病變，讓他們深入查看掃描結果，減少 80% 的損失。&lt;/li&gt;
&lt;li&gt;作者將異常檢測任務制定為修復(in-painting)任務，使能夠利用放射線照相影像的外觀、位置和佈局的解剖一致性。&lt;/li&gt;
&lt;li&gt;異常檢測的成功有兩個基本假設：首先，數據中很少出現異常；其次，異常現象與正常模式顯著不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;squid-作用&#34;&gt;
    &lt;a href=&#34;#squid-%e4%bd%9c%e7%94%a8&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    SQUID 作用
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;訓練期間，模型可以透過根據空間位置，對經常出現的解剖模式進行分類來動態維護visual pattern dictionary(視覺模式字典)。&lt;/li&gt;
&lt;li&gt;由於解剖學的consistency(一致性)，健康影像中相同身體區域預計會表達相似的視覺模式，使得unique pattern的總數易於管理(類型不會太多)。&lt;/li&gt;
&lt;li&gt;推理期間，由於無監督指使用健康影像訓練，故異常模式不存在於字典中。當出現異常時，產生的影像就會有所差距。也因此模型可透過區分修復任務的品質來識別異常。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;實驗簡介&#34;&gt;
    &lt;a href=&#34;#%e5%af%a6%e9%a9%97%e7%b0%a1%e4%bb%8b&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    實驗簡介
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;在兩個大規模、公開的放射線照相成像資料集上進行了實驗&lt;/li&gt;
&lt;li&gt;ZhangLab dataset: 發現 SQUID 在無監督異常檢測方面明顯優於主流方法超過 5 個百分點。&lt;/li&gt;
&lt;li&gt;Stanford CheXpert dataset: 證明了比最近 13 種無監督異常檢測方法提高了 10 個百分點。&lt;/li&gt;
&lt;li&gt;創建了一個新的資料集（DigitAnatomy）
&lt;ul&gt;
&lt;li&gt;闡明放射攝影中胸部解剖結構的空間相關性和一致形狀&lt;/li&gt;
&lt;li&gt;致力於簡化異常檢測方法的開發、評估和解釋&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;貢獻&#34;&gt;
    &lt;a href=&#34;#%e8%b2%a2%e7%8d%bb&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    貢獻
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;用於胸部放射成像的最佳無監督異常檢測方法&lt;/li&gt;
&lt;li&gt;促進異常檢測研究的綜合資料集(DigitAnatomy dataset)&lt;/li&gt;
&lt;li&gt;SQUID 發明 Space-aware Memory Queue（3.2 節）和 Feature-level In-painting （3.3 節），打敗了主流無監督異常檢測方法的限制。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;2-related-work&#34;&gt;
    &lt;a href=&#34;#2-related-work&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    2. Related Work
&lt;/h1&gt;&lt;h2 id=&#34;anomaly-detection-in-natural-imaging&#34;&gt;
    &lt;a href=&#34;#anomaly-detection-in-natural-imaging&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Anomaly detection in natural imaging
&lt;/h2&gt;&lt;h3 id=&#34;過往的異常檢測&#34;&gt;
    &lt;a href=&#34;#%e9%81%8e%e5%be%80%e7%9a%84%e7%95%b0%e5%b8%b8%e6%aa%a2%e6%b8%ac&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    過往的異常檢測
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Reconstruction-based methods: 基於重建的方法訓練模型（例如Auto-Encoder）來恢復原始輸入，並透過分析重建錯誤來識別異常。&lt;/li&gt;
&lt;li&gt;Density-based methods: 估計常態資料分佈（例如透過 VAE 或 GAN）來預測異常。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而，透過這些方法學習到的正常影像的分佈無法解釋可能的異常。&lt;/p&gt;
&lt;h3 id=&#34;解決無法解釋可能的異常限制&#34;&gt;
    &lt;a href=&#34;#%e8%a7%a3%e6%b1%ba%e7%84%a1%e6%b3%95%e8%a7%a3%e9%87%8b%e5%8f%af%e8%83%bd%e7%9a%84%e7%95%b0%e5%b8%b8%e9%99%90%e5%88%b6&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    解決&amp;quot;無法解釋可能的異常&amp;quot;限制
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;本篇作者透過&lt;strong&gt;維護同質醫學影像的 visual pattern memory&lt;/strong&gt;來解決。&lt;/li&gt;
&lt;li&gt;先前的其他幾項工作研究了使用影像修復進行異常檢測，即輸入影像的部分內容被屏蔽，並且訓練模型以自我監督的方式恢復遺失的部分。&lt;/li&gt;
&lt;li&gt;還有大量關於檢測影片序列中的異常的工作。伯格曼等人。和薩利希等人提出了類似的&lt;strong&gt;學生-教師網絡&lt;/strong&gt;，而我們的方法利用這種結構僅提取輸入感知特徵，並且&lt;strong&gt;教師網絡在推理過程中完全禁用&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;anomaly-detection-in-medical-imaging&#34;&gt;
    &lt;a href=&#34;#anomaly-detection-in-medical-imaging&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Anomaly detection in medical imaging
&lt;/h2&gt;&lt;h3 id=&#34;兩種學習方式檢測不同異常&#34;&gt;
    &lt;a href=&#34;#%e5%85%a9%e7%a8%ae%e5%ad%b8%e7%bf%92%e6%96%b9%e5%bc%8f%e6%aa%a2%e6%b8%ac%e4%b8%8d%e5%90%8c%e7%95%b0%e5%b8%b8&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    兩種學習方式，檢測不同異常
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;基於監督學習的方法: 通常用於檢測特定類型的異常，例如病變、病理、腫瘤和結節。&lt;/li&gt;
&lt;li&gt;無監督方法: 檢測一般異常。多為基於 GAN 的做法，但是這些方法需要有關於異常種類的強大先驗知識和假設才能使增強有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;攝影影像-vs-放射成像&#34;&gt;
    &lt;a href=&#34;#%e6%94%9d%e5%bd%b1%e5%bd%b1%e5%83%8f-vs-%e6%94%be%e5%b0%84%e6%88%90%e5%83%8f&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    攝影影像 vs 放射成像
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;放射成像協議產生具有一致解剖模式的影像，由於微妙的成像線索和重疊的解剖結構，檢測起來更具挑戰性。&lt;/li&gt;
&lt;li&gt;作者提出了一種新穎的方法，可以明確地利用放射線照相影像的屬性，顯著提高放射線照相影像異常檢測的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;memory-networks&#34;&gt;
    &lt;a href=&#34;#memory-networks&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Memory networks
&lt;/h2&gt;&lt;p&gt;過往研究證明，在神經網路中納入記憶模組是有效的。然而現有方法需要不少額外記憶體。&lt;/p&gt;
&lt;p&gt;在本文中，作者克服了記憶體矩陣的局限性，並提出了一種有效且高效的&lt;strong&gt;memory queue&lt;/strong&gt;，用於放射線照相影像中的無監督異常檢測。&lt;/p&gt;
&lt;h1 id=&#34;3-squid&#34;&gt;
    &lt;a href=&#34;#3-squid&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3. SQUID
&lt;/h1&gt;&lt;h2 id=&#34;31-overview&#34;&gt;
    &lt;a href=&#34;#31-overview&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3.1. Overview
&lt;/h2&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig1.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig1.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;
    &lt;a href=&#34;#feature-extraction&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Feature extraction
&lt;/h3&gt;&lt;p&gt;將輸入影像分割成 NxN 個不重疊區塊，將他們輸入 encoder 做特徵提取，這些提取出的特徵用於影像重建。&lt;/p&gt;
&lt;p&gt;而這個 encoder 可以是任何骨幹架構，簡單起見，此處用的是基本的捲積和池化層。&lt;/p&gt;
&lt;h3 id=&#34;image-reconstruction&#34;&gt;
    &lt;a href=&#34;#image-reconstruction&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Image reconstruction
&lt;/h3&gt;&lt;p&gt;引入【teacher and student generators】重建原始影像。隨著重建，解剖模式(anatomical patterns)的字典將作為 &lt;strong&gt;Memory Queue&lt;/strong&gt; 動態建立和更新。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teacher generators: 使用前面 encoder(auto-encoder) 提取出的特徵直接重建影像。充當正規化器，防止學生不斷生成相同的正常影像。&lt;/li&gt;
&lt;li&gt;Student generators: 則是使用後面會提到的 &lt;strong&gt;in-painting block&lt;/strong&gt; 增強方法所獲得的特徵。Student generators 目標是根據增強的特徵重建出影像，用於異常辨識。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;teacher and student generators 使用知識蒸餾在 up-smapling 層級上進行耦合。&lt;/p&gt;
&lt;h3 id=&#34;anomaly-discrimination&#34;&gt;
    &lt;a href=&#34;#anomaly-discrimination&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Anomaly discrimination
&lt;/h3&gt;&lt;p&gt;作者採用 discriminator 評估產生的影像是真 or 假。且只有 &lt;strong&gt;Student generators&lt;/strong&gt; 才會收到從鑑別器導出的梯度(只用來更新Student generators)。兩個生成器和鑑別器相互競爭，直到收斂到平衡。&lt;/p&gt;
&lt;p&gt;經過訓練後，&lt;strong&gt;鑑別器可用於偵測測試影像中的異常&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;32-inventing-memory-queue-as-dictionary&#34;&gt;
    &lt;a href=&#34;#32-inventing-memory-queue-as-dictionary&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3.2. Inventing Memory Queue as Dictionary
&lt;/h2&gt;&lt;h3 id=&#34;motivation&#34;&gt;
    &lt;a href=&#34;#motivation&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Motivation
&lt;/h3&gt;&lt;p&gt;為了打造「正常」外觀，透過對相似的 pattern 進行加權平均來對特徵做增強。然而，這種增強應用於從整個影像中提取的特徵，會丟棄影像中的空間資訊。因此，當前形式的 Memory Matrix(記憶矩陣) 無法感知放射影像中的解剖一致性。&lt;/p&gt;
&lt;h3 id=&#34;space-aware-memory&#34;&gt;
    &lt;a href=&#34;#space-aware-memory&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Space-aware memory
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;為了&lt;strong&gt;獲取空間資訊&lt;/strong&gt;，將分割的 patches 而不是整個影像傳遞到模型中。&lt;/li&gt;
&lt;li&gt;特定位置的 patch 只能存取整個 Memory Matrix 中的對應段，以此建立特定關係。稱為【&lt;strong&gt;Space-aware memory&lt;/strong&gt;】。&lt;/li&gt;
&lt;li&gt;可加快增強速度，因為它不再透過整個 Memory Matrix 來組裝類似的特徵。
&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig2.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig2.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memory-queue&#34;&gt;
    &lt;a href=&#34;#memory-queue&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Memory queue
&lt;/h3&gt;&lt;p&gt;在 learning-based Memory Matrix，normal patterns 是結合 matrix basis 形成的，但這種組合和實際影像特徵之間總是存在分佈差異，使後續影像生成變得困難。&lt;/p&gt;
&lt;p&gt;作者提出 Memory queue 在模型訓練期間&lt;strong&gt;儲存真實影像特徵&lt;/strong&gt;，從而呈現與影像特徵相同的分佈，他在訓練期間直接將先前看到的特徵複製到 queue 中。訓練後，Memory queue 可當作正常解剖 pattern 的字典。&lt;/p&gt;
&lt;h3 id=&#34;gumbel-shrinkage&#34;&gt;
    &lt;a href=&#34;#gumbel-shrinkage&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Gumbel shrinkage
&lt;/h3&gt;&lt;p&gt;控制 Memory matrix 中 activated patterns 的數量有利於異常檢測。然而，設定 hard shrinkage threshold 無法處理記憶體中找不到合適 entries(條目)的情況。&lt;/p&gt;
&lt;p&gt;一種自然的解法是將梯度流過記憶體中的 top-k 個相似 pattern。然而，其餘未啟動的條目無法接收任何梯度並按預期進行更新。因此提出了 Gumbel shrinkage schema:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$w&amp;rsquo; = sg(hs(w; topk(w))-ϕ(w))+ϕ(w)$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;w 代表影像 feature 和 entry 間的相似度&lt;/li&gt;
&lt;li&gt;sg(⋅) 代表 stop-gradient，不計算輸入的梯度&lt;/li&gt;
&lt;li&gt;hs(⋅,t) 代表閾值 t 的 hard shrinkage&lt;/li&gt;
&lt;li&gt;ϕ(⋅) 代表 softmax&lt;/li&gt;
&lt;li&gt;這樣確保  memory 中 top-k 個相似 entries 的組合 ，又用 softmax 對所有 entry 進行更新&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作者將其應用於框架中的 memory queue 和 memory matrix。&lt;/p&gt;
&lt;h2 id=&#34;33-formulating-anomaly-detection-as-inpainting&#34;&gt;
    &lt;a href=&#34;#33-formulating-anomaly-detection-as-inpainting&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3.3. Formulating Anomaly Detection as Inpainting
&lt;/h2&gt;&lt;h3 id=&#34;motivation-1&#34;&gt;
    &lt;a href=&#34;#motivation-1&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Motivation
&lt;/h3&gt;&lt;p&gt;Image in-painting(影像修復) 最初是為了恢復具有鄰近上下文的損壞的影像區域而被提出。因此，作者將異常射線照相圖案修復到健康射線照相圖案中來實現異常檢測。&lt;/p&gt;
&lt;p&gt;修復的影像區域通常被認為與 boundary artifacts(邊界偽影)相關，當進行像素級修復任務時，不良偽影會導致大量誤報。因此作者&lt;strong&gt;在特徵層級實現了修復任務&lt;/strong&gt;。潛在特徵對像素級雜訊、旋轉和平移具有更好的不變性，因此更適合後續的異常檢測。&lt;/p&gt;
&lt;h3 id=&#34;in-painting-block&#34;&gt;
    &lt;a href=&#34;#in-painting-block&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    In-painting block
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig3.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig3.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作者將 Memory Queue 整合到一個新穎的 in-painting block 中以執行特徵空間修復，修復三步驟如圖。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;【Fig(a)】 : 在 queue 中增強 $w×h$ 個非重疊 patch 特徵 $F_{{(1,1),&amp;hellip;,(w, h)}}$ 到最相似的 normal patterns $N_{{(1,1),&amp;hellip;,(w, h)}}$。&lt;/li&gt;
&lt;li&gt;【Fig(b)】 : 由於 $N$ 是由從先前的訓練資料中提取的特徵組成，因此 $N$ 不受當前輸入影像的影響。為了回顧輸入影像的特徵，我們使用 transformer block 來聚合 patch 特徵 $F$ 及其增強特徵 $N$。
&lt;ul&gt;
&lt;li&gt;對於每個 patch $F_{i,j}$，其相鄰的八個增強區塊 $N_{(i-1,j-1),&amp;hellip;,(i+1,j+1)}$ 被用做細化(refine) $F_{i,j}$ 的條件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transformer block 的 query token 被展平為 $F_{i,j}∈R^{1×*}$ ； 而 key/value tokens 則展平為 $N_{(i-1,j-1),&amp;hellip;,(i+1,j+1)}∈R^{8×*}$。&lt;/p&gt;
&lt;p&gt;在 in-painting block 的開始和結束處，作者用了一對額外的 point-wise convolutions（1×1 卷積核）。&lt;/p&gt;
&lt;h3 id=&#34;masked-shortcut&#34;&gt;
    &lt;a href=&#34;#masked-shortcut&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Masked shortcut
&lt;/h3&gt;&lt;p&gt;作者在 in-painting block 中使用，以此&lt;strong&gt;更好地聚合特徵並簡化最佳化&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;作者實證表明，direct residual connection（直接殘差連接）會降低 in-painting block 的有效性，因此在訓練期間利用　random binary mask　來門控快捷特徵【Fig(b)】。給定輸入 patch 特徵 $F$，in-painting block 的輸出透過以下方式獲得：&lt;/p&gt;
&lt;p&gt;$F&amp;rsquo;=(1-δ)⋅F+δ⋅inpaint(F)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inpaint(·): 設計的 in-painting block&lt;/li&gt;
&lt;li&gt;δ ~ Bernoulli(ρ): 具有閘控機率(gating probability)的二元變數&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在每個訓練步獲得 $F&amp;rsquo;$ 後，原始的 $F$ 被複製來更新記憶體【Fig(c)】。在推理過程中，完全 shortcut，以便 $F&amp;rsquo; = inpaint(F)$ 進行確定性預測。&lt;/p&gt;
&lt;h2 id=&#34;34-anomaly-discrimination&#34;&gt;
    &lt;a href=&#34;#34-anomaly-discrimination&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3.4 Anomaly Discrimination
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;鑑別器評估重建影像，若判定不現實表示發生異常。&lt;/li&gt;
&lt;li&gt;因生成器以正常影像上進行訓練，memory queue 僅儲存 normal pattern。因此在推理中，abnormal pattern 重建的影像預計會顯得不真實。&lt;/li&gt;
&lt;li&gt;小結
&lt;ul&gt;
&lt;li&gt;in-painting block 專注在將任何 patch(正常或異常) 特徵增強為類似的“正常”特徵。&lt;/li&gt;
&lt;li&gt;student generator 據“正常”特徵重建“正常”圖像&lt;/li&gt;
&lt;li&gt;teacher generator 用於防止學生無論輸入如何都產生相同的圖像。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;沒異常的話，input 和 teacher generator 重建的圖像在語意上應該相差很小。因此，我們委託優化的鑑別器網路來感知異常警報，異常分數計算如下。&lt;/p&gt;
&lt;p&gt;$A=ϕ(\frac{D(G_{s}(E(I)))-μ}{σ})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E: encoder&lt;/li&gt;
&lt;li&gt;$G_t$: teacher generator&lt;/li&gt;
&lt;li&gt;$G_s$: student generator&lt;/li&gt;
&lt;li&gt;D: discriminator&lt;/li&gt;
&lt;li&gt;A: 異常分數&lt;/li&gt;
&lt;li&gt;ϕ(⋅): sigmoid function&lt;/li&gt;
&lt;li&gt;μ 和 σ 是根據 training samples 算出的異常分數的平均值和標準差&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;35-loss-function&#34;&gt;
    &lt;a href=&#34;#35-loss-function&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    3.5. Loss Function
&lt;/h2&gt;&lt;p&gt;共有五種，自己看論文 :D&lt;/p&gt;
&lt;h1 id=&#34;4-experiments&#34;&gt;
    &lt;a href=&#34;#4-experiments&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    4. Experiments
&lt;/h1&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/table1.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/table1.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;h2 id=&#34;41-new-benchmark&#34;&gt;
    &lt;a href=&#34;#41-new-benchmark&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    4.1. New Benchmark
&lt;/h2&gt;&lt;h3 id=&#34;digitanatomy數位解剖學&#34;&gt;
    &lt;a href=&#34;#digitanatomy%e6%95%b8%e4%bd%8d%e8%a7%a3%e5%89%96%e5%ad%b8&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    DigitAnatomy(數位解剖學)
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig4.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig4.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;作者創建了一個合成資料集，圖片中有數字 1~9，數字順序正確被視為「正常」；否則異常。異常包括缺失、亂序、翻轉和 zero digit。&lt;/p&gt;
&lt;p&gt;該資料集對於放射線成像尤其有利&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模擬其兩個獨特屬性，即空間相關性和一致形狀。&lt;/li&gt;
&lt;li&gt;無需生物專業知識，更容易進行問題調試。&lt;/li&gt;
&lt;li&gt;容易獲得模擬異常的 ground truth。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;42-public-benchmarks&#34;&gt;
    &lt;a href=&#34;#42-public-benchmarks&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    4.2. Public Benchmarks
&lt;/h2&gt;&lt;p&gt;ZhangLab Chest X-ray&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;包含健康和肺炎（作為異常）影像&lt;/li&gt;
&lt;li&gt;訓練集
&lt;ul&gt;
&lt;li&gt;1349 張正常&lt;/li&gt;
&lt;li&gt;3883 張異常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;測試集
&lt;ul&gt;
&lt;li&gt;234 張正常&lt;/li&gt;
&lt;li&gt;390 張異常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;作者從訓練集中隨機選 200 張影像（100 張正常和 100 張異常影像）作為超參數調整的驗證集。&lt;/li&gt;
&lt;li&gt;所有影像的大小調整為 128x128。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stanford CheXpert&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對 front-view PA 影像進行評估，共有 12 種異常&lt;/li&gt;
&lt;li&gt;訓練集
&lt;ul&gt;
&lt;li&gt;5249 張正常&lt;/li&gt;
&lt;li&gt;23671 張異常&lt;/li&gt;
&lt;li&gt;使用和 ZhangLab 相同的超參數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;測試集
&lt;ul&gt;
&lt;li&gt;用訓練集的 250 張正常和 250 張異常進行測試&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;43-baselines-and-metrics&#34;&gt;
    &lt;a href=&#34;#43-baselines-and-metrics&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    4.3. Baselines and Metrics
&lt;/h2&gt;&lt;p&gt;13 個 baseline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;經典 UAD(unsupervised anomaly detection) 方法
&lt;ul&gt;
&lt;li&gt;Auto-Encoder, VAE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;醫學影像的 SOTA
&lt;ul&gt;
&lt;li&gt;Ganomaly、f-AnoGAN、IF、SALAD、&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最新的 UAD 方法
&lt;ul&gt;
&lt;li&gt;emAE、CutPaste、M-KD、PANDA、PaDiM、IGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;標準指標評&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Receiver Operating Characteristic (ROC) curve&lt;/li&gt;
&lt;li&gt;ROC 曲線下面積 (AUC)、&lt;/li&gt;
&lt;li&gt;準確度 (Acc)&lt;/li&gt;
&lt;li&gt;F1 分數 (F1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除非有特別註明，不然都是從頭獨立訓練模型至少三次&lt;/p&gt;
&lt;h1 id=&#34;5-results&#34;&gt;
    &lt;a href=&#34;#5-results&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    5. Results
&lt;/h1&gt;&lt;h2 id=&#34;51-interpreting-squid-on-digitanatomy&#34;&gt;
    &lt;a href=&#34;#51-interpreting-squid-on-digitanatomy&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    5.1. Interpreting SQUID on DigitAnatomy
&lt;/h2&gt;&lt;p&gt;承前面數字圖可見，SQUID 重建的圖像比其他方法更有意義和指示性。這主要歸因於 Space-aware memory，由此產生的字典與獨特的模式及其空間訊息相關聯。&lt;/p&gt;
&lt;p&gt;一旦出現異常（例如丟失數字），in-painting block 將透過從字典中組裝前 top-k 個最相似的 pattern 來將異常特徵增強到其正常對應特徵。&lt;/p&gt;
&lt;p&gt;GAN 傾向於重建訓練樣本平均得到的影像。MemAE 由於其 memory matrix 而表現相對較好，但對於缺失數字的異常效果不佳，在極端異常攻擊上完全失敗。&lt;/p&gt;
&lt;h2 id=&#34;52-benchmarking-squid-on-chest-radiography&#34;&gt;
    &lt;a href=&#34;#52-benchmarking-squid-on-chest-radiography&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    5.2. Benchmarking SQUID on Chest Radiography
&lt;/h2&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig5.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig5.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;圖中視覺化了 SQUID 在正常和異常影像上的重建結果。正常情況下，SQUID可以很容易地在 Memory Queue 中找到相似的匹配，順利實現重建；對於異常情況，將偽造的正常 pattern 強加到異常特徵中就會產生矛盾。這樣，生成的圖像將與輸入顯著不同，然後由鑑別器捕獲。&lt;/p&gt;
&lt;h3 id=&#34;limitation&#34;&gt;
    &lt;a href=&#34;#limitation&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Limitation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目前形式的 SQUID 無法在像素層級精確定位異常。&lt;/li&gt;
&lt;li&gt;因為 SQUID 是一種無監督方法，需要對正常/異常影像進行零手動註釋。&lt;/li&gt;
&lt;li&gt;那些像素級別的異常檢測會遭遇放大雜訊的影響，但是由於 SQUID 是在特徵層級進行的，比像素級別更加 robust。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;53-ablating消融-key-properties-in-squid&#34;&gt;
    &lt;a href=&#34;#53-ablating%e6%b6%88%e8%9e%8d-key-properties-in-squid&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    5.3. Ablating(消融) Key Properties in SQUID
&lt;/h2&gt;&lt;h3 id=&#34;component-study&#34;&gt;
    &lt;a href=&#34;#component-study&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Component study
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/table2.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/table2.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&#34;hyper-parameter-robustness&#34;&gt;
    &lt;a href=&#34;#hyper-parameter-robustness&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Hyper-parameter robustness
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig6.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig6.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&#34;disease-free-training-requirement&#34;&gt;
    &lt;a href=&#34;#disease-free-training-requirement&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Disease-free training requirement?
&lt;/h3&gt;&lt;p&gt;&lt;div class=&#34;post-img-view&#34;&gt;
        &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig7.png&#34;&gt;
            &lt;img src=&#34;https://charlieUWUuwu.github.io/img/paper/SQUID/fig7.png&#34; loading=&#34;lazy&#34; alt=&#34;image&#34;  /&gt;
        &lt;/a&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;p&gt;用於醫學異常檢測的無監督方法並不常見，因為所謂的 UAD 方法不是「無監督的」—— 它們必須僅在無疾病影像上進行訓練。&lt;/p&gt;
&lt;p&gt;在實踐中，要獲得健康圖片需要人工標註。&lt;/p&gt;
&lt;p&gt;在訓練集中考慮 disease-free 從 100% - 50% 的情況，把 SQUID 的 robust 和另外三個 baseline 進行比較。&lt;/p&gt;
&lt;p&gt;從圖中可知，SQUID 的 memory queue 可以自動忽略來自少數的 anatomical patterns，容忍高達 50% 的疾病/健康訓練比率。&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;
    &lt;a href=&#34;#conclusion&#34; class=&#34;header-anchor&#34;&gt;#&lt;/a&gt;
    Conclusion
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;提出 SQUID，用於從放射線影像中進行無監督異常檢測&lt;/li&gt;
&lt;li&gt;SQUID 可以將根深蒂固的解剖結構分類為 recurrent pattern(固定重複出現的 pattern)&lt;/li&gt;
&lt;li&gt;在推理中，SQUID可以準確地識別異常情況&lt;/li&gt;
&lt;li&gt;SQUID 在ZhangLab 資料集上優於主流方法超過 5 點 AUC，在史丹佛 CheXpert 資料集上優於主流方法 10 點 AUC&lt;/li&gt;
&lt;li&gt;合成了 DigitAnatomy 資料集，以類似於放射線照相影像中胸部解剖結構的關鍵屬性&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
