[{"content":" # 說明 就像 ChatGPT 那樣一個字一個字跑出來。這個方法是在 LLaMA-Factory 看到的。 完整 code 可以在這個專案裡面找\n# 用 HuggingFace 模型做流式輸出 HuggingFace 的 v4.30.1 加入了兩個流式輸出的接口\nTextStreamer: 能在 stdout 中流式輸出结果 TextIteratorStreamer：能在自定義 loop 中進行操作 # TextStreamer 1# 來自 https://huggingface.co/docs/transformers/main/generation_strategies 2 3from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer 4 5tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) 6model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) 7inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) 8streamer = TextStreamer(tok) 9 10# Despite returning the usual output, the streamer will also print the generated text to stdout. 11_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20) # TextIteratorStreamer 1# 來自 https://huggingface.co/docs/transformers/main/generation_strategies 2 3from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer 4from threading import Thread 5 6tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) 7model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) 8inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) 9streamer = TextIteratorStreamer(tok) 10 11# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way. 12generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20) 13thread = Thread(target=model.generate, kwargs=generation_kwargs) 14thread.start() 15generated_text = \u0026#34;\u0026#34; 16for new_text in streamer: 17 generated_text += new_text 18generated_text # 用 ChatGLM 示範 參考 [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 中的「ChatGLM流式回覆Demo」部分\n# 讓 OpenAI 模型也能流式輸出 參考 Huggingface 官方實現的 BaseStreamer 寫出來的。\n# 設定 OpenAI 模型 streaming設置為True，可以不用等一句話完全生完後才得到回覆 StreamingStdOutCallbackHandler 是 Langchain 的一個 callback 處理器，會在語言模型模型生成新 token 時被觸發(on_llm_new_token())，並透過sys.stdout.write(token)與sys.stdout.flush()來確保輸出即時可見。 說明可見這個部落格 1from langchain_openai import ChatOpenAI 2from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler 3 4model = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0) # 撰寫 ChatgptStreamer 主要是透過 Python 的 Iterator 來實現的。\n_on_finalized_text() : 將收到的 text 存到 queue，直到收到 stream 結束訊號(stream_end==True)， model_generate() : 接收語言模型並調用.stream()來獲得流式輸出，最後後塞入stop_signal表輸出結束。 1from queue import Queue 2 3class ChatgptStreamer: 4 def __init__(self, timeout: float=3): 5 self.text_queue = Queue() 6 self.stop_signal = None 7 self.timeout = timeout 8 9 def _on_finalized_text(self, text: str, stream_end: bool = False): 10 \u0026#34;\u0026#34;\u0026#34;Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\u0026#34;\u0026#34;\u0026#34; 11 self.text_queue.put(text, timeout=self.timeout) 12 if stream_end: 13 self.text_queue.put(self.stop_signal, timeout=self.timeout) 14 15 def model_generate(self, model, system, query): 16 for chunk in model.stream(f\u0026#34;請你根據以下參考資料回答問題。\\n相關資料:{system}\\n問題:{query}\u0026#34;): 17 self._on_finalized_text(chunk.content) 18 self._on_finalized_text(self.stop_signal, True) 19 20 def __iter__(self): 21 return self 22 23 def __next__(self): 24 value = self.text_queue.get(timeout=self.timeout) 25 if value == self.stop_signal: 26 raise StopIteration() 27 else: 28 return value # Streamer+Thread 進行生成 1@torch.inference_mode() 2def stream_chat( 3 self, 4 query: str, 5 history: Optional[List[Tuple[str, str]]] = None, 6 system: Optional[str] = None, 7 **input_kwargs 8) -\u0026gt; Generator[str, None, None]: 9 gen_kwargs, _ = self._process_args(query, history, system, **input_kwargs) 10 if gen_kwargs == {}: 11 # 使用 ChatOpenAI 12 streamer = ChatgptStreamer(timeout=3) 13 thread = Thread(target=streamer.model_generate, args=(self.model, system, query)) 14 else: 15 # 使用 Huggingface 模型 16 streamer = TextIteratorStreamer(self.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True) 17 gen_kwargs[\u0026#34;streamer\u0026#34;] = streamer 18 19 thread = Thread(target=self.model.generate, kwargs=gen_kwargs) 20 21 thread.start() 22 yield from streamer # 使用 1def predict( 2 self, 3 chatbot: List[Tuple[str, str]], 4 query: str, 5 history: List[Tuple[str, str]], 6) -\u0026gt; Generator[Tuple[List[Tuple[str, str]], List[Tuple[str, str]]], None, None]: 7 chatbot.append([query, \u0026#34;\u0026#34;]) 8 print(\u0026#34;chatbot : \u0026#34;, chatbot) 9 response = \u0026#34;\u0026#34; 10 # 從 VDB 獲取資料 11 system = \u0026#34;\u0026#34; 12 docs = self.vectordb_manager.query(query, n_results=5) # 回傳最相關的 5 筆相關資料 13 for doc in docs: 14 system += doc.page_content 15 print(\u0026#34;系統提示詞 : \u0026#34;, system) 16 17 for new_text in self.stream_chat( 18 query, None, system 19 ): 20 response += new_text 21 chatbot[-1] = [query, self.postprocess(response)] 22 yield chatbot, history # refs : [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 HuggingFace 官方文件 ","date":"2024-04-10T03:30:04+08:00","permalink":"https://charlieUWUuwu.github.io/p/chat_streamer/","title":"Chat_streamer"},{"content":" # SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection paper : https://arxiv.org/abs/2111.13495\n# Abstract 射線照相成像協議專注於特定的身體區域，因此產生非常相似的影像並產生跨患者的重複解剖結構。\n為了利用這種結構化訊息，使用 Space-aware Memory Queues for In-painting and Detecting（SQUID）來修復和偵測射線照相影像中的異常。\n證明 SQUID 可以將根深蒂固的解剖結構分類為循環模式(recurrent patterns)。(它可以把固有的人體結構分類為反覆出現的 pattern)\n在推理中，它可以識別圖像中的異常（未見/修改的模式）。\n作者創建了一個新的資料集（DigitAnatomy），綜合了胸部解剖學中的空間相關性和一致的形狀。\n# 1. Introduction # 放射成像 vs 一般影像 普通視覺任務有平移不變性的假設(貓出現在左或是右，還是貓)；但放射線成像(radiography imaging)，結構的相對位置和方向是識別的重要特徵。 且因放射成像在不同患者或設備商評估方式幾乎相似，故一致且重複的解剖結構有助於分析許多關鍵問題，並應被視為放射線成像的顯著優勢。 # 放射成像的先驗知識 有多項研究顯示添加位置特徵、修改目標函數以及約束相對於影像中地標的座標能增強深度網路效能。此偏論文希望能利用一致的解剖模式及其空間資訊，加強深度網路對放射線影像異常的檢測，而無需手動註釋。\n# 醫療異常檢測作用 80% 的臨床錯誤是由於放射科醫師漏掉異常造成 異常檢測作用是向放射科醫生明確指出存在可疑病變，讓他們深入查看掃描結果，減少 80% 的損失。 作者將異常檢測任務制定為修復(in-painting)任務，使能夠利用放射線照相影像的外觀、位置和佈局的解剖一致性。 異常檢測的成功有兩個基本假設：首先，數據中很少出現異常；其次，異常現象與正常模式顯著不同。 # SQUID 作用 訓練期間，模型可以透過根據空間位置，對經常出現的解剖模式進行分類來動態維護visual pattern dictionary(視覺模式字典)。 由於解剖學的consistency(一致性)，健康影像中相同身體區域預計會表達相似的視覺模式，使得unique pattern的總數易於管理(類型不會太多)。 推理期間，由於無監督指使用健康影像訓練，故異常模式不存在於字典中。當出現異常時，產生的影像就會有所差距。也因此模型可透過區分修復任務的品質來識別異常。 # 實驗簡介 在兩個大規模、公開的放射線照相成像資料集上進行了實驗 ZhangLab dataset: 發現 SQUID 在無監督異常檢測方面明顯優於主流方法超過 5 個百分點。 Stanford CheXpert dataset: 證明了比最近 13 種無監督異常檢測方法提高了 10 個百分點。 創建了一個新的資料集（DigitAnatomy） 闡明放射攝影中胸部解剖結構的空間相關性和一致形狀 致力於簡化異常檢測方法的開發、評估和解釋 # 貢獻 用於胸部放射成像的最佳無監督異常檢測方法 促進異常檢測研究的綜合資料集(DigitAnatomy dataset) SQUID 發明 Space-aware Memory Queue（3.2 節）和 Feature-level In-painting （3.3 節），打敗了主流無監督異常檢測方法的限制。 # 2. Related Work # Anomaly detection in natural imaging # 過往的異常檢測 Reconstruction-based methods: 基於重建的方法訓練模型（例如Auto-Encoder）來恢復原始輸入，並透過分析重建錯誤來識別異常。 Density-based methods: 估計常態資料分佈（例如透過 VAE 或 GAN）來預測異常。 然而，透過這些方法學習到的正常影像的分佈無法解釋可能的異常。\n# 解決\u0026quot;無法解釋可能的異常\u0026quot;限制 本篇作者透過維護同質醫學影像的 visual pattern memory來解決。 先前的其他幾項工作研究了使用影像修復進行異常檢測，即輸入影像的部分內容被屏蔽，並且訓練模型以自我監督的方式恢復遺失的部分。 還有大量關於檢測影片序列中的異常的工作。伯格曼等人。和薩利希等人提出了類似的學生-教師網絡，而我們的方法利用這種結構僅提取輸入感知特徵，並且教師網絡在推理過程中完全禁用。 # Anomaly detection in medical imaging # 兩種學習方式，檢測不同異常 基於監督學習的方法: 通常用於檢測特定類型的異常，例如病變、病理、腫瘤和結節。 無監督方法: 檢測一般異常。多為基於 GAN 的做法，但是這些方法需要有關於異常種類的強大先驗知識和假設才能使增強有效。 # 攝影影像 vs 放射成像 放射成像協議產生具有一致解剖模式的影像，由於微妙的成像線索和重疊的解剖結構，檢測起來更具挑戰性。 作者提出了一種新穎的方法，可以明確地利用放射線照相影像的屬性，顯著提高放射線照相影像異常檢測的性能。 # Memory networks 過往研究證明，在神經網路中納入記憶模組是有效的。然而現有方法需要不少額外記憶體。\n在本文中，作者克服了記憶體矩陣的局限性，並提出了一種有效且高效的memory queue，用於放射線照相影像中的無監督異常檢測。\n# 3. SQUID # 3.1. Overview # Feature extraction 將輸入影像分割成 NxN 個不重疊區塊，將他們輸入 encoder 做特徵提取，這些提取出的特徵用於影像重建。\n而這個 encoder 可以是任何骨幹架構，簡單起見，此處用的是基本的捲積和池化層。\n# Image reconstruction 引入【teacher and student generators】重建原始影像。隨著重建，解剖模式(anatomical patterns)的字典將作為 Memory Queue 動態建立和更新。\nTeacher generators: 使用前面 encoder(auto-encoder) 提取出的特徵直接重建影像。充當正規化器，防止學生不斷生成相同的正常影像。 Student generators: 則是使用後面會提到的 in-painting block 增強方法所獲得的特徵。Student generators 目標是根據增強的特徵重建出影像，用於異常辨識。 teacher and student generators 使用知識蒸餾在 up-smapling 層級上進行耦合。\n# Anomaly discrimination 作者採用 discriminator 評估產生的影像是真 or 假。且只有 Student generators 才會收到從鑑別器導出的梯度(只用來更新Student generators)。兩個生成器和鑑別器相互競爭，直到收斂到平衡。\n經過訓練後，鑑別器可用於偵測測試影像中的異常。\n# 3.2. Inventing Memory Queue as Dictionary # Motivation 為了打造「正常」外觀，透過對相似的 pattern 進行加權平均來對特徵做增強。然而，這種增強應用於從整個影像中提取的特徵，會丟棄影像中的空間資訊。因此，當前形式的 Memory Matrix(記憶矩陣) 無法感知放射影像中的解剖一致性。\n# Space-aware memory 為了獲取空間資訊，將分割的 patches 而不是整個影像傳遞到模型中。 特定位置的 patch 只能存取整個 Memory Matrix 中的對應段，以此建立特定關係。稱為【Space-aware memory】。 可加快增強速度，因為它不再透過整個 Memory Matrix 來組裝類似的特徵。 # Memory queue 在 learning-based Memory Matrix，normal patterns 是結合 matrix basis 形成的，但這種組合和實際影像特徵之間總是存在分佈差異，使後續影像生成變得困難。\n作者提出 Memory queue 在模型訓練期間儲存真實影像特徵，從而呈現與影像特徵相同的分佈，他在訓練期間直接將先前看到的特徵複製到 queue 中。訓練後，Memory queue 可當作正常解剖 pattern 的字典。\n# Gumbel shrinkage 控制 Memory matrix 中 activated patterns 的數量有利於異常檢測。然而，設定 hard shrinkage threshold 無法處理記憶體中找不到合適 entries(條目)的情況。\n一種自然的解法是將梯度流過記憶體中的 top-k 個相似 pattern。然而，其餘未啟動的條目無法接收任何梯度並按預期進行更新。因此提出了 Gumbel shrinkage schema:\n$w\u0026rsquo; = sg(hs(w; topk(w))-ϕ(w))+ϕ(w)$\nw 代表影像 feature 和 entry 間的相似度 sg(⋅) 代表 stop-gradient，不計算輸入的梯度 hs(⋅,t) 代表閾值 t 的 hard shrinkage ϕ(⋅) 代表 softmax 這樣確保 memory 中 top-k 個相似 entries 的組合 ，又用 softmax 對所有 entry 進行更新 作者將其應用於框架中的 memory queue 和 memory matrix。\n# 3.3. Formulating Anomaly Detection as Inpainting # Motivation Image in-painting(影像修復) 最初是為了恢復具有鄰近上下文的損壞的影像區域而被提出。因此，作者將異常射線照相圖案修復到健康射線照相圖案中來實現異常檢測。\n修復的影像區域通常被認為與 boundary artifacts(邊界偽影)相關，當進行像素級修復任務時，不良偽影會導致大量誤報。因此作者在特徵層級實現了修復任務。潛在特徵對像素級雜訊、旋轉和平移具有更好的不變性，因此更適合後續的異常檢測。\n# In-painting block 作者將 Memory Queue 整合到一個新穎的 in-painting block 中以執行特徵空間修復，修復三步驟如圖。\n【Fig(a)】 : 在 queue 中增強 $w×h$ 個非重疊 patch 特徵 $F_{{(1,1),\u0026hellip;,(w, h)}}$ 到最相似的 normal patterns $N_{{(1,1),\u0026hellip;,(w, h)}}$。 【Fig(b)】 : 由於 $N$ 是由從先前的訓練資料中提取的特徵組成，因此 $N$ 不受當前輸入影像的影響。為了回顧輸入影像的特徵，我們使用 transformer block 來聚合 patch 特徵 $F$ 及其增強特徵 $N$。 對於每個 patch $F_{i,j}$，其相鄰的八個增強區塊 $N_{(i-1,j-1),\u0026hellip;,(i+1,j+1)}$ 被用做細化(refine) $F_{i,j}$ 的條件。 Transformer block 的 query token 被展平為 $F_{i,j}∈R^{1×*}$ ； 而 key/value tokens 則展平為 $N_{(i-1,j-1),\u0026hellip;,(i+1,j+1)}∈R^{8×*}$。\n在 in-painting block 的開始和結束處，作者用了一對額外的 point-wise convolutions（1×1 卷積核）。\n# Masked shortcut 作者在 in-painting block 中使用，以此更好地聚合特徵並簡化最佳化。\n作者實證表明，direct residual connection（直接殘差連接）會降低 in-painting block 的有效性，因此在訓練期間利用　random binary mask　來門控快捷特徵【Fig(b)】。給定輸入 patch 特徵 $F$，in-painting block 的輸出透過以下方式獲得：\n$F\u0026rsquo;=(1-δ)⋅F+δ⋅inpaint(F)$\ninpaint(·): 設計的 in-painting block δ ~ Bernoulli(ρ): 具有閘控機率(gating probability)的二元變數 在每個訓練步獲得 $F\u0026rsquo;$ 後，原始的 $F$ 被複製來更新記憶體【Fig(c)】。在推理過程中，完全 shortcut，以便 $F\u0026rsquo; = inpaint(F)$ 進行確定性預測。\n# 3.4 Anomaly Discrimination 鑑別器評估重建影像，若判定不現實表示發生異常。 因生成器以正常影像上進行訓練，memory queue 僅儲存 normal pattern。因此在推理中，abnormal pattern 重建的影像預計會顯得不真實。 小結 in-painting block 專注在將任何 patch(正常或異常) 特徵增強為類似的“正常”特徵。 student generator 據“正常”特徵重建“正常”圖像 teacher generator 用於防止學生無論輸入如何都產生相同的圖像。 沒異常的話，input 和 teacher generator 重建的圖像在語意上應該相差很小。因此，我們委託優化的鑑別器網路來感知異常警報，異常分數計算如下。\n$A=ϕ(\\frac{D(G_{s}(E(I)))-μ}{σ})$\nE: encoder $G_t$: teacher generator $G_s$: student generator D: discriminator A: 異常分數 ϕ(⋅): sigmoid function μ 和 σ 是根據 training samples 算出的異常分數的平均值和標準差 # 3.5. Loss Function 共有五種，自己看論文 :D\n# 4. Experiments # 4.1. New Benchmark # DigitAnatomy(數位解剖學) 作者創建了一個合成資料集，圖片中有數字 1~9，數字順序正確被視為「正常」；否則異常。異常包括缺失、亂序、翻轉和 zero digit。\n該資料集對於放射線成像尤其有利\n模擬其兩個獨特屬性，即空間相關性和一致形狀。 無需生物專業知識，更容易進行問題調試。 容易獲得模擬異常的 ground truth。 # 4.2. Public Benchmarks ZhangLab Chest X-ray\n包含健康和肺炎（作為異常）影像 訓練集 1349 張正常 3883 張異常 測試集 234 張正常 390 張異常 作者從訓練集中隨機選 200 張影像（100 張正常和 100 張異常影像）作為超參數調整的驗證集。 所有影像的大小調整為 128x128。 Stanford CheXpert\n對 front-view PA 影像進行評估，共有 12 種異常 訓練集 5249 張正常 23671 張異常 使用和 ZhangLab 相同的超參數 測試集 用訓練集的 250 張正常和 250 張異常進行測試 # 4.3. Baselines and Metrics 13 個 baseline\n經典 UAD(unsupervised anomaly detection) 方法 Auto-Encoder, VAE 醫學影像的 SOTA Ganomaly、f-AnoGAN、IF、SALAD、 最新的 UAD 方法 emAE、CutPaste、M-KD、PANDA、PaDiM、IGD 標準指標評\nReceiver Operating Characteristic (ROC) curve ROC 曲線下面積 (AUC)、 準確度 (Acc) F1 分數 (F1) 除非有特別註明，不然都是從頭獨立訓練模型至少三次\n# 5. Results # 5.1. Interpreting SQUID on DigitAnatomy 承前面數字圖可見，SQUID 重建的圖像比其他方法更有意義和指示性。這主要歸因於 Space-aware memory，由此產生的字典與獨特的模式及其空間訊息相關聯。\n一旦出現異常（例如丟失數字），in-painting block 將透過從字典中組裝前 top-k 個最相似的 pattern 來將異常特徵增強到其正常對應特徵。\nGAN 傾向於重建訓練樣本平均得到的影像。MemAE 由於其 memory matrix 而表現相對較好，但對於缺失數字的異常效果不佳，在極端異常攻擊上完全失敗。\n# 5.2. Benchmarking SQUID on Chest Radiography 圖中視覺化了 SQUID 在正常和異常影像上的重建結果。正常情況下，SQUID可以很容易地在 Memory Queue 中找到相似的匹配，順利實現重建；對於異常情況，將偽造的正常 pattern 強加到異常特徵中就會產生矛盾。這樣，生成的圖像將與輸入顯著不同，然後由鑑別器捕獲。\n# Limitation 目前形式的 SQUID 無法在像素層級精確定位異常。 因為 SQUID 是一種無監督方法，需要對正常/異常影像進行零手動註釋。 那些像素級別的異常檢測會遭遇放大雜訊的影響，但是由於 SQUID 是在特徵層級進行的，比像素級別更加 robust。 # 5.3. Ablating(消融) Key Properties in SQUID # Component study # Hyper-parameter robustness # Disease-free training requirement? 用於醫學異常檢測的無監督方法並不常見，因為所謂的 UAD 方法不是「無監督的」—— 它們必須僅在無疾病影像上進行訓練。\n在實踐中，要獲得健康圖片需要人工標註。\n在訓練集中考慮 disease-free 從 100% - 50% 的情況，把 SQUID 的 robust 和另外三個 baseline 進行比較。\n從圖中可知，SQUID 的 memory queue 可以自動忽略來自少數的 anatomical patterns，容忍高達 50% 的疾病/健康訓練比率。\n# Conclusion 提出 SQUID，用於從放射線影像中進行無監督異常檢測 SQUID 可以將根深蒂固的解剖結構分類為 recurrent pattern(固定重複出現的 pattern) 在推理中，SQUID可以準確地識別異常情況 SQUID 在ZhangLab 資料集上優於主流方法超過 5 點 AUC，在史丹佛 CheXpert 資料集上優於主流方法 10 點 AUC 合成了 DigitAnatomy 資料集，以類似於放射線照相影像中胸部解剖結構的關鍵屬性 ","date":"2024-04-10T03:30:04+08:00","permalink":"https://charlieUWUuwu.github.io/p/squid/","title":"SQUID"}]