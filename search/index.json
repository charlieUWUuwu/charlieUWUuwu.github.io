[{"content":" # 操作指令 # 建立本地項目 # 使用 Vite npm create vite@latest my-project1 或是\nnpm create vite # 使用 create-react-app npx create-react-app # 進入專案資料夾 cd my-project1 # 安裝額外包(如React) npm install # 啟動開發預覽 server npm run dev # vite npm start # create-react-app # Udemy 教材啟用起手式 npm install npm run dev # 使用 redux # 安裝工具箱 npm install react-redux npm install @reduxjs/toolkit # 安裝開發工具 Google 搜尋 Redux devtool ","date":"2024-07-06T20:33:33+08:00","permalink":"https://charlieUWUuwu.github.io/p/react-%E5%9F%BA%E7%A4%8E%E6%8C%87%E4%BB%A4/","title":"React 基礎指令"},{"content":" # Github 基礎指令 # 下載 git # 確認有安裝 git $ git --version $ cd ./專案所在資料夾 # 初次使用 使用者設定 $ git config --global user.name \u0026#34;YOUR_NAME\u0026#34; $ git config --global user.email \u0026#34;YOUR_EMAIL\u0026#34; 查看使用者訊息 $ git config --list user.name=YOUR_NAME user.email=YOUR_EMAIL # Git 使用 初始 Repository $ git init 觀察 Repository 檔案追蹤狀況 $ git status 將檔案加入暫存區(Staging Area) # 添加一份檔案 $ git add welcome.html # 添加所有檔案 $ git add . # 再次檢查 $ git status 查看暫存區資料 $ git ls-files git add 反悔 (清除暫存區檔案) $ git reset HEAD -- [路徑] 將暫存區(Staging Area)檔案提交到倉庫(Repository) $ git commit -m \u0026#34;提交訊息紀錄\u0026#34; # -m 表示直接在命令行中提供提交訊息，後面的 \u0026#34;init commit\u0026#34; 是具體的提交訊息內容。 commit 反悔 $ git rm -r --cached [路徑] # 分支 branch 印出目前這個專案有哪些分支 $ git branch 新增+切換分支\n法 1 新增分支 $ git branch \u0026lt;分支名\u0026gt; # 執行結束過後，目前還是會在 master 的主要分支上。 切換分支 $ git checkout cat Switched to branch \u0026#39;cat\u0026#39; # 切換分支，若不存在分支就創建一個 $ git checkout -b sister Switched to a new branch \u0026#39;sister\u0026#39; 法 2 新建分支並切換過去 $ git checkout -b \u0026lt;分支名\u0026gt; 分支改名字\n$ git branch -m cat tiger # cat -\u0026gt; tiger 合併分支 # 切回主要分支 $ git checkout master Switched to branch \u0026#39;master\u0026#39; # 將分支 cat 合併到 master $ git merge cat ✨: 合併過的分支不會消失，要不要留著取決於自己~\n刪除分支 $ git branch -d cat # GitHub 使用 # 開始前，可以先檢測目前與 Github 的 SSH 連線狀況 $ ssh -T git@github.com 如果沒有設定過 SSH 連線，則會返回 Permission denied 的錯誤訊息，如下：\ngit@github.com: Permission denied (publickey).\n# Git 設定 SSH 連線 產生金鑰 $ ssh-keygen -t rsa -b 4096 -C \u0026#34;你的E-mail\u0026#34; # 也可直接打 ssh-keygen 進入金鑰資料夾查看 $ cd /c/Users/[userName]/.ssh # 通常會有 id_rsa、id_rsa.pub 兩個檔案 到 GitHub 設置鑰匙 開啟GitHub，登入後點頭像的 Settings 點擊左邊 Access 的 SSH keys and GPG keys 點選 New SSH key 回到 git 畫面，輸入指令查看id_rsa.pub裡面的資料，並將顯示的資料填回 github 的key欄位。其中 Title 的部分可寫可不寫 $ cat id_rsa.pub 查看 ssh 連線是否成功，中間要輸入密碼 $ ssh -T git@github.com # 阻止特定檔案被 push 到 GitHub 上 新增 .gitignore 檔案 內容放要被忽略的檔案類型 例如，想要忽略所有 .log 檔案、/model 資料夾 *.log /model 重新提交到暫存區 # 清除本機 Git 的快取，就是將所有檔案移除 Git 的追蹤，但沒有刪除檔案 $ git rm -r --cached . # 重新加入 Git 追蹤，這時就會重新套入 .gitignore 設定 $ git add . # 重新 commit ，並會忽略設定在 .gitignore 的檔案 $ git commit -m \u0026#39;update .gitignore\u0026#39; # Push 到 GitHub 上 進入 GitHub 建立新的 repository\n說明： 如果是全新開始，請依「create a new repository on the command line」的內容指示進行；如果是要上傳現存專案，則依照「push an existing repository from the command line」選項進行。 可以看到在 Quick start 那邊有 HTTPS 跟 SSH 兩個，若用 HTTPS 的話在登入時會需要輸入帳號密碼，而使用 SSH 的話則需要使用密鑰來登入，以下介紹 SSH 的使用。 獲取 ssh 連線地址 進入 Repository 點擊 設定遠端節點\n$ git remote add origin \u0026lt;ssh 連線地址\u0026gt; - 說明： 1. git remote 指令，顧名思義，主要是跟遠端有關的操作。 2. add 指令是指要加入一個遠端的節點。 3. 在這裡的 origin 是一個「代名詞」，指的是後面那串 GitHub 伺服器的位置。 查詢遠端節點 $ git remote -v # 出現以下 origin git@github.com:charlieUWUuwu/practice-git2.git (fetch) origin git@github.com:charlieUWUuwu/practice-git2.git (push) 查詢遠端節點分支 $ git branch -r 更改遠端節點的位置 ✨: 若傳案起初是從別人那邊 clone 過來的，記得執行這邊，將 url 改到自己的倉庫\n$ git remote set-url origin \u0026lt;新網址\u0026gt; 刪除遠端節點 # $ git remote rm origin $ git remote rm \u0026lt;遠端節點名\u0026gt; 資料推到github上 #$ git push -u origin master $ git push -u \u0026lt;遠端節點名\u0026gt; \u0026lt;branch名\u0026gt; ✨: 檔案不能超過 100M，否則後續 push 會一直失敗\n解決辦法 : https://www.twblogs.net/a/5ca39948bd9eee5b1a06eede 注意! Windows 系統要用 \u0026quot; 而不是 \u0026lsquo;，否則會出現 rm 錯誤啥的 # 從 GitHub 上 Pull 下來 ✨: 以下是建立在【自己已經有這個專案】的前提下\n跟 Push 指令相反，Pull 指令是拉回本機更新。\nPull 下載更新 git pull = git fetch + git merge 上線抓東西下來（Fetch），並且更新本機的進度（Merge）。 $ git pull # Clone : 第一次使用專案時，抓取遠端檔案 Clone 指令會把整個專案的內容複製一份到你的電腦裡，這裡指的「內容」不是只有檔案，而是指所有整個專案的歷史紀錄、分支、標籤等內容都會複製一份下來。\n:bulb: 以下使用 [SSH] 的連結\n$ git clone git@github.com:kaochenlong/dummy-git.git # 若要 Clone 下來之後存成不同的目錄名稱 $ git clone git@github.com:kaochenlong/dummy-git.git hello_kitty # Pull Request（PR）: 與其它開發者的互動 在 GitHub 專案畫面上有Fork跟Pull Request按鈕可以按 Fork : 複製別人的專案到自己帳號底下 Pull Request : 修改完東西後，發通知給原作者問他要不要 pull 你的專案(還記得pull裡面有merge的功能嗎，如果原作者pull你就表示他要跟你的合併)\n也建議看一下這個 Git + GitHub 版本控制教學 (5) - 使用 GitHub 與團隊合作\n# 和朋朋協作專案 借用一下之前和同學比賽的內容\n# How to maintain a repo 每次增修內容前請依循下列流程進行：\nPull origin/develop 最新版本 $ git pull origin develop 在 local 新增 branch 並切換 $ git checkout -b \u0026lt;NEW_BRANCH_NAME\u0026gt; 編輯完成後進行 commit $ git add . $ git commit -m \u0026#34;COMMIT_MSG\u0026#34; 回到 develop 再次獲取 origin/develop 的最新版本、與自己的修正合併並修正出現的 conflict $ git checkout develop $ git pull $ git checkout \u0026lt;NEW_BRANCH_NAME\u0026gt; $ git rebase develop 將新 branch 的修正與 develop 合併並 push 到 Github $ git checkout develop $ git merge \u0026lt;NEW_BRANCH_NAME\u0026gt; $ git push # Contributing Guidelines 以下講解 branch 命名，以及撰寫 commit 的一種規則。 後也可以先看 git commit 都怎麼寫的\n# Git Branch Naming Except for permanent branches (e.g. main and develop), all temporary branches should be named in the format \u0026lt;TYPE\u0026gt;/\u0026lt;ISSUE\u0026gt;-\u0026lt;DETAIL\u0026gt; such as feat/1-integer-datatype. If the goal of a branch has not related to any issues, you can either create an issue to describe the detail of your planning or skip the \u0026lt;ISSUE\u0026gt;- in the branch name. With this convention, we can understand the purpose of each branch from its name directly.\nThere are several catagories that are available for \u0026lt;TYPE\u0026gt;:\nfeat bug docs # Git Commit Message Convention There are some common commit types:\nfeat: A new feature. fix: A bug to be fixed. style: Changes about coding style without changing the behavior. BREAKING CHANGE perf: Improve the performance of a functionality. refactor: Restructure the implementation of a functionality without changing its behavior. revert: Revert a previous commit without other changes. chore: Changes about build tools or project configurations. test: Add or correct tests. docs: Update documentations only. update: update document (not code)、 db、 json、 md ","date":"2024-06-03T21:46:25+08:00","permalink":"https://charlieUWUuwu.github.io/p/github-%E5%9F%BA%E7%A4%8E%E6%8C%87%E4%BB%A4/","title":"Github 基礎指令"},{"content":"先介紹 PGGAN \u0026gt; StyleGAN \u0026gt; pSp\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. # pix2style2pix(pSp) 論文閱讀 paper: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\n# Abstract 先前 styleGAN 已經可以生成高品質的人臉，且可以控制圖像特定特徵(完成特徵解纏)，但主要是針對生成器架構做改進，因此還不能進行 img2img 任務。而 pSp 進行了以下改善 :\n提出通用框架 pixel2style2pixel(pSp)，實現 image-to-image 新的 encoder + 預訓練 styleGAN generator(decoder) 提出新的 encoder 架構 基於特徵金字塔網路 (Feature Pyramid Network) 能夠產生 style vector 並輸入到預訓練的 StyleGAN 生成器，形成 extended $\\textit{W+}$ latent space 不需進行耗時最佳化 不需要使用成對訓練資料 透過對 styleGAN 做 resample，進行多模態合成 訓練 encoder 時引入 ID loss，避免影像重建後變了一個人(更精準的控制細節) # Introduction 先前 styleGAN 是「invert first, edit later」(先反轉到latent space，再生成影像)，但將真實影像反轉為 512 維向量 w ∈ W 不能做準確的重建。而 pSp 將真實影像編碼到 W+，也就是直接將輸入影像編碼為對應的輸出潛在影像，從而允許處理不屬於 StyleGAN 域的輸入。\nW+ latent space 由 18 個不同的 512 維 w 向量串聯(18x512x1)，且每一個向量會送給 styleGAN 每一個輸入層，但單一影像用此方法優化就會需要好幾分鐘。因此將真實影像快速且準確地反演為 W+ 仍然是一個挑戰。\n遵循 pix2pix 精神，定義能夠解決所有任務的通用架構。pix2style2pix 表示先將圖像用 encoder 編碼成 style vector，然後再用 decoer 轉回圖像。\n不需對手鑑別器 (adversary discriminator)，而直接使用預訓練 styleGAN，因此有以下優勢 :\n生成器僅受 style 控制，沒有直接的空間輸入 intermediate style 適合模糊任務的【多模態合成】 (例如 sketch2img、影像高清化等，可以做髮色改變之類的)。這是因為可以對生成的 style 重新採樣來創建輸出圖像的變體。 # The pSp Framework 使用一個強大的 encoder，能將每個輸入影像與 latent domain 中的準確編碼相匹配。將圖像嵌入到 latent domain 有一些做法\n(傳統) 從 encoder 最後一層獲得單一 512 維向量，直接 repeat 18 次成為 W+，從而一起學習所有 18 個風格向量。但難以表示原始影像的精細細節，導致重建品質受限。 (pSp) 先前 styleGAN 的特徵依據分層結構的深淺，代表不同粒度的特徵。pSp 依此出發，使用特徵金字塔擴展 encoder 主幹，產生三個層級的特徵圖，從全連接層(map2style)提取 style(如上圖) 產生 latent code(512*1)。接著輸入到生成器中對應的地方以產生影像。 這邊將輸入定義為 :\n$pSp(x):=G(E(x)+\\bar{w})$\nx: 輸入影像 E(·) 和 G(·): encoder 跟 StyleGAN 生成器 公式中，encoder 旨在學習相對於 average style vector 的 latent code。\n# Loss Functions encoder 的訓練是透過多個 loss 的加權組合而成的。\n$\\mathcal{L_2}(x)=|x-pSp(x)|_2$\npixel-wise L2 loss $\\mathcal{L}_{PIPS}(x)=|F(x)-F(pSp(x))|_2$\n為了學習感知相似性，利用 LPIPS 損失(已被證明可以更好地保持影像品質) $\\mathcal{L}_{reg}(x)=|E(x)-\\bar{w}|_2$\n鼓勵編碼器輸出更接近平均latent vector的latent styke vector，另外定義的正規化損失 與 StyleGAN 中的截斷技巧類似，作者發現在encoder的訓練中添加這種正則化可以提高圖像品質，而不損害輸出的保真度，特別是在更模糊的任務中 $\\mathcal{L}_{ID}(x)=1-\u0026lt;R(x),\\ R(pSp(x))\u0026gt;$\n處理臉部影像編碼的特定任務時的一個常見挑戰是保存輸入身分，因此採用專用的辨識損失來測量輸出影像與其來源影像之間的餘弦相似度 R: 預訓練的 ArcFace 模型 $\\mathcal{L}(x)=\\lambda_1\\mathcal{L}2(x)+\\lambda_2\\mathcal{L}{PIPS}(x)+\\lambda_3\\mathcal{L}{ID}(x)+\\lambda_4\\mathcal{L}{reg}(x)$\n最終結果。其中$\\lambda_1$~$\\lambda_4$是定義損失權重的常數。 # The Benefits of The StyleGAN Domain 透過 style domain 做圖像轉換使 pSp 在全局而非local運行，因此不需要像 pix2pix 的對應。\n由於某些翻譯任務是不明確的，其中單一輸入影像可能對應於多個輸出，因此希望能夠對這些可能的輸出進行取樣。\n前人文獻提到特徵解纏(語義分離)是因為 layer-wise representation 帶來的，因為能獨立操作特徵，自然就會想要「多模態數據生成」。\n論文做法如下\n圖片經過 encoder 生成 latent code(18x512x1)，輸入給 style mixing 網路內的 1~7 層使用 隨機採樣向量 $w ∈ R^{512}$ ，並複製 w 在 W+ 中產生相應的 latent code，輸入給style mixing 網路內的剩下層。 可以加入參數來控制兩者的融合，在 pSp 中是在 coarse 跟 medium 層面上使用圖像經 encode 輸出的 style，fine 才使用隨機變量，以保證產生的是同一個人，但有不同的細節變化。\n# Applications and Experiments 在許多 img2img 的任務尚實驗，確定方法的有效性。\n# StyleGAN Inversion 在 latent domain 中找到 real image 的 leatent code。也就是要比較 encoder 轉換後的 latent code，能否有比較準確的生成效果。\n結果 :\nALAE 方法在 W 域中運行，無法準確地重建輸入影像。 雖然 IDInvert 更好地保留了影像屬性，但它仍然無法準確保留輸入影像的身份和更精細的細節。相較之下，pSp 能夠保留身份，同時重建燈光、髮型和眼鏡等細節。 ✨ IDInvert中，首先將要轉換的圖像編碼成潛在向量（latent code），這個潛在向量是StyleGAN預訓練模型的潛在空間中的一個點。然後，通過對生成的圖像進行直接優化，調整這個潛在向量，使生成的圖像盡可能地接近原始圖像。這樣做的目的是在不重新訓練整個StyleGAN模型的情況下，將圖像轉換到StyleGAN的潛在空間中。\n接著做 pSp 架構的消融實驗\nW : 從 encoder 最後一層提取出在 512 維的 style vector(屬於 W latent domain) Naive W+ : 承上，接著使用具有附加層的 encoder，將 1x512 特徵向量轉為 18x512 的 W+ 向量。 結果 :\nW+ 擴展顯著改善了結果，但仍然無法保留更精細的細節。 有 ID loss 能夠帶來更好的生分特徵(重建後不會變一個人)\n接下來就是對不同影像生成任務情景做實驗觀察，詳見論文。\n# Face Frontalization 由於需要非局部轉換並且缺乏配對訓練數據，臉部正面化對於影像到影像轉換框架來說是一項具有挑戰性的任務。 略\n# Conditional Image Synthesis 條件影像合成旨在產生基於某些輸入類型的逼真影像。在本節中，我們的 pSp 架構在兩個條件影像生成任務上進行了測試。 從草圖和語意分割圖產生高品質的人臉影像。 我們證明，只需進行最小的更改，我們的編碼器就可以成功利用 StyleGAN 的表達能力來產生高品質和多樣化的輸出。\n# Face From Sketch 草圖到真實圖過往需要成對資料，然而 pSp 不需要。\n作者團隊建立了草圖影像資料集，有很多人臉手繪插圖\n作者觀察到 pSp 獲得更多樣化輸出的能力，這些輸出可以更好地保留更精細的細節（例如臉部毛髮）。\n# Discussion pSp 仍然有一些固有假設(inherent assumptions):\n產生圖像受限 因為使用預訓練 styleGAN，所以能夠生成的圖像僅限於 styleGAN 可以產生的圖像。 目前仍難以保留輸入影像的更精細細節(見上圖) pSp 的全局方法在不同任務上的通用性高，但還無法保留輸入影像的更精細細節（例如耳環或背景細節） 這對於影像修復等任務很重要 ✨ 其他實驗見論文拉\n# Ref pixel2style2pixel（pSp）实现解读 ","date":"2024-04-26T00:06:37+08:00","permalink":"https://charlieUWUuwu.github.io/p/pixel2style2pixel/","title":"Pixel2style2pixel"},{"content":" # 說明 就像 ChatGPT 那樣一個字一個字跑出來。這個方法是在 LLaMA-Factory 看到的。 完整 code 可以在這個專案裡面找\n# 用 HuggingFace 模型做流式輸出 HuggingFace 的 v4.30.1 加入了兩個流式輸出的接口\nTextStreamer: 能在 stdout 中流式輸出结果 TextIteratorStreamer：能在自定義 loop 中進行操作 # TextStreamer # 來自 https://huggingface.co/docs/transformers/main/generation_strategies from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) streamer = TextStreamer(tok) # Despite returning the usual output, the streamer will also print the generated text to stdout. _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20) # TextIteratorStreamer # 來自 https://huggingface.co/docs/transformers/main/generation_strategies from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer from threading import Thread tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) streamer = TextIteratorStreamer(tok) # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way. generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20) thread = Thread(target=model.generate, kwargs=generation_kwargs) thread.start() generated_text = \u0026#34;\u0026#34; for new_text in streamer: generated_text += new_text generated_text # 用 ChatGLM 示範 參考 [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 中的「ChatGLM流式回覆Demo」部分\n# 讓 OpenAI 模型也能流式輸出 參考 Huggingface 官方實現的 BaseStreamer 寫出來的。\n# 設定 OpenAI 模型 streaming設置為True，可以不用等一句話完全生完後才得到回覆 StreamingStdOutCallbackHandler 是 Langchain 的一個 callback 處理器，會在語言模型模型生成新 token 時被觸發(on_llm_new_token())，並透過sys.stdout.write(token)與sys.stdout.flush()來確保輸出即時可見。 說明可見這個部落格 from langchain_openai import ChatOpenAI from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler model = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0) # 撰寫 ChatgptStreamer 主要是透過 Python 的 Iterator 來實現的。\n_on_finalized_text() : 將收到的 text 存到 queue，直到收到 stream 結束訊號(stream_end==True)， model_generate() : 接收語言模型並調用.stream()來獲得流式輸出，最後後塞入stop_signal表輸出結束。 from queue import Queue class ChatgptStreamer: def __init__(self, timeout: float=3): self.text_queue = Queue() self.stop_signal = None self.timeout = timeout def _on_finalized_text(self, text: str, stream_end: bool = False): \u0026#34;\u0026#34;\u0026#34;Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\u0026#34;\u0026#34;\u0026#34; self.text_queue.put(text, timeout=self.timeout) if stream_end: self.text_queue.put(self.stop_signal, timeout=self.timeout) def model_generate(self, model, system, query): for chunk in model.stream(f\u0026#34;請你根據以下參考資料回答問題。\\n相關資料:{system}\\n問題:{query}\u0026#34;): self._on_finalized_text(chunk.content) self._on_finalized_text(self.stop_signal, True) def __iter__(self): return self def __next__(self): value = self.text_queue.get(timeout=self.timeout) if value == self.stop_signal: raise StopIteration() else: return value # Streamer+Thread 進行生成 @torch.inference_mode() def stream_chat( self, query: str, history: Optional[List[Tuple[str, str]]] = None, system: Optional[str] = None, **input_kwargs ) -\u0026gt; Generator[str, None, None]: gen_kwargs, _ = self._process_args(query, history, system, **input_kwargs) if gen_kwargs == {}: # 使用 ChatOpenAI streamer = ChatgptStreamer(timeout=3) thread = Thread(target=streamer.model_generate, args=(self.model, system, query)) else: # 使用 Huggingface 模型 streamer = TextIteratorStreamer(self.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True) gen_kwargs[\u0026#34;streamer\u0026#34;] = streamer thread = Thread(target=self.model.generate, kwargs=gen_kwargs) thread.start() yield from streamer # 使用 def predict( self, chatbot: List[Tuple[str, str]], query: str, history: List[Tuple[str, str]], ) -\u0026gt; Generator[Tuple[List[Tuple[str, str]], List[Tuple[str, str]]], None, None]: chatbot.append([query, \u0026#34;\u0026#34;]) print(\u0026#34;chatbot : \u0026#34;, chatbot) response = \u0026#34;\u0026#34; # 從 VDB 獲取資料 system = \u0026#34;\u0026#34; docs = self.vectordb_manager.query(query, n_results=5) # 回傳最相關的 5 筆相關資料 for doc in docs: system += doc.page_content print(\u0026#34;系統提示詞 : \u0026#34;, system) for new_text in self.stream_chat( query, None, system ): response += new_text chatbot[-1] = [query, self.postprocess(response)] yield chatbot, history # refs : [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 HuggingFace 官方文件 ","date":"2024-04-10T03:30:04+08:00","permalink":"https://charlieUWUuwu.github.io/p/chat_streamer/","title":"Chat_streamer"},{"content":" # SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection paper : https://arxiv.org/abs/2111.13495\n# Abstract 射線照相成像協議專注於特定的身體區域，因此產生非常相似的影像並產生跨患者的重複解剖結構。\n為了利用這種結構化訊息，使用 Space-aware Memory Queues for In-painting and Detecting（SQUID）來修復和偵測射線照相影像中的異常。\n證明 SQUID 可以將根深蒂固的解剖結構分類為循環模式(recurrent patterns)。(它可以把固有的人體結構分類為反覆出現的 pattern)\n在推理中，它可以識別圖像中的異常（未見/修改的模式）。\n作者創建了一個新的資料集（DigitAnatomy），綜合了胸部解剖學中的空間相關性和一致的形狀。\n# 1. Introduction # 放射成像 vs 一般影像 普通視覺任務有平移不變性的假設(貓出現在左或是右，還是貓)；但放射線成像(radiography imaging)，結構的相對位置和方向是識別的重要特徵。 且因放射成像在不同患者或設備商評估方式幾乎相似，故一致且重複的解剖結構有助於分析許多關鍵問題，並應被視為放射線成像的顯著優勢。 # 放射成像的先驗知識 有多項研究顯示添加位置特徵、修改目標函數以及約束相對於影像中地標的座標能增強深度網路效能。此偏論文希望能利用一致的解剖模式及其空間資訊，加強深度網路對放射線影像異常的檢測，而無需手動註釋。\n# 醫療異常檢測作用 80% 的臨床錯誤是由於放射科醫師漏掉異常造成 異常檢測作用是向放射科醫生明確指出存在可疑病變，讓他們深入查看掃描結果，減少 80% 的損失。 作者將異常檢測任務制定為修復(in-painting)任務，使能夠利用放射線照相影像的外觀、位置和佈局的解剖一致性。 異常檢測的成功有兩個基本假設：首先，數據中很少出現異常；其次，異常現象與正常模式顯著不同。 # SQUID 作用 訓練期間，模型可以透過根據空間位置，對經常出現的解剖模式進行分類來動態維護visual pattern dictionary(視覺模式字典)。 由於解剖學的consistency(一致性)，健康影像中相同身體區域預計會表達相似的視覺模式，使得unique pattern的總數易於管理(類型不會太多)。 推理期間，由於無監督指使用健康影像訓練，故異常模式不存在於字典中。當出現異常時，產生的影像就會有所差距。也因此模型可透過區分修復任務的品質來識別異常。 # 實驗簡介 在兩個大規模、公開的放射線照相成像資料集上進行了實驗 ZhangLab dataset: 發現 SQUID 在無監督異常檢測方面明顯優於主流方法超過 5 個百分點。 Stanford CheXpert dataset: 證明了比最近 13 種無監督異常檢測方法提高了 10 個百分點。 創建了一個新的資料集（DigitAnatomy） 闡明放射攝影中胸部解剖結構的空間相關性和一致形狀 致力於簡化異常檢測方法的開發、評估和解釋 # 貢獻 用於胸部放射成像的最佳無監督異常檢測方法 促進異常檢測研究的綜合資料集(DigitAnatomy dataset) SQUID 發明 Space-aware Memory Queue（3.2 節）和 Feature-level In-painting （3.3 節），打敗了主流無監督異常檢測方法的限制。 # 2. Related Work # Anomaly detection in natural imaging # 過往的異常檢測 Reconstruction-based methods: 基於重建的方法訓練模型（例如Auto-Encoder）來恢復原始輸入，並透過分析重建錯誤來識別異常。 Density-based methods: 估計常態資料分佈（例如透過 VAE 或 GAN）來預測異常。 然而，透過這些方法學習到的正常影像的分佈無法解釋可能的異常。\n# 解決\u0026quot;無法解釋可能的異常\u0026quot;限制 本篇作者透過維護同質醫學影像的 visual pattern memory來解決。 先前的其他幾項工作研究了使用影像修復進行異常檢測，即輸入影像的部分內容被屏蔽，並且訓練模型以自我監督的方式恢復遺失的部分。 還有大量關於檢測影片序列中的異常的工作。伯格曼等人。和薩利希等人提出了類似的學生-教師網絡，而我們的方法利用這種結構僅提取輸入感知特徵，並且教師網絡在推理過程中完全禁用。 # Anomaly detection in medical imaging # 兩種學習方式，檢測不同異常 基於監督學習的方法: 通常用於檢測特定類型的異常，例如病變、病理、腫瘤和結節。 無監督方法: 檢測一般異常。多為基於 GAN 的做法，但是這些方法需要有關於異常種類的強大先驗知識和假設才能使增強有效。 # 攝影影像 vs 放射成像 放射成像協議產生具有一致解剖模式的影像，由於微妙的成像線索和重疊的解剖結構，檢測起來更具挑戰性。 作者提出了一種新穎的方法，可以明確地利用放射線照相影像的屬性，顯著提高放射線照相影像異常檢測的性能。 # Memory networks 過往研究證明，在神經網路中納入記憶模組是有效的。然而現有方法需要不少額外記憶體。\n在本文中，作者克服了記憶體矩陣的局限性，並提出了一種有效且高效的memory queue，用於放射線照相影像中的無監督異常檢測。\n# 3. SQUID # 3.1. Overview # Feature extraction 將輸入影像分割成 NxN 個不重疊區塊，將他們輸入 encoder 做特徵提取，這些提取出的特徵用於影像重建。\n而這個 encoder 可以是任何骨幹架構，簡單起見，此處用的是基本的捲積和池化層。\n# Image reconstruction 引入【teacher and student generators】重建原始影像。隨著重建，解剖模式(anatomical patterns)的字典將作為 Memory Queue 動態建立和更新。\nTeacher generators: 使用前面 encoder(auto-encoder) 提取出的特徵直接重建影像。充當正規化器，防止學生不斷生成相同的正常影像。 Student generators: 則是使用後面會提到的 in-painting block 增強方法所獲得的特徵。Student generators 目標是根據增強的特徵重建出影像，用於異常辨識。 teacher and student generators 使用知識蒸餾在 up-smapling 層級上進行耦合。\n# Anomaly discrimination 作者採用 discriminator 評估產生的影像是真 or 假。且只有 Student generators 才會收到從鑑別器導出的梯度(只用來更新Student generators)。兩個生成器和鑑別器相互競爭，直到收斂到平衡。\n經過訓練後，鑑別器可用於偵測測試影像中的異常。\n# 3.2. Inventing Memory Queue as Dictionary # Motivation 為了打造「正常」外觀，透過對相似的 pattern 進行加權平均來對特徵做增強。然而，這種增強應用於從整個影像中提取的特徵，會丟棄影像中的空間資訊。因此，當前形式的 Memory Matrix(記憶矩陣) 無法感知放射影像中的解剖一致性。\n# Space-aware memory 為了獲取空間資訊，將分割的 patches 而不是整個影像傳遞到模型中。 特定位置的 patch 只能存取整個 Memory Matrix 中的對應段，以此建立特定關係。稱為【Space-aware memory】。 可加快增強速度，因為它不再透過整個 Memory Matrix 來組裝類似的特徵。 # Memory queue 在 learning-based Memory Matrix，normal patterns 是結合 matrix basis 形成的，但這種組合和實際影像特徵之間總是存在分佈差異，使後續影像生成變得困難。\n作者提出 Memory queue 在模型訓練期間儲存真實影像特徵，從而呈現與影像特徵相同的分佈，他在訓練期間直接將先前看到的特徵複製到 queue 中。訓練後，Memory queue 可當作正常解剖 pattern 的字典。\n# Gumbel shrinkage 控制 Memory matrix 中 activated patterns 的數量有利於異常檢測。然而，設定 hard shrinkage threshold 無法處理記憶體中找不到合適 entries(條目)的情況。\n一種自然的解法是將梯度流過記憶體中的 top-k 個相似 pattern。然而，其餘未啟動的條目無法接收任何梯度並按預期進行更新。因此提出了 Gumbel shrinkage schema:\n$w\u0026rsquo; = sg(hs(w; topk(w))-ϕ(w))+ϕ(w)$\nw 代表影像 feature 和 entry 間的相似度 sg(⋅) 代表 stop-gradient，不計算輸入的梯度 hs(⋅,t) 代表閾值 t 的 hard shrinkage ϕ(⋅) 代表 softmax 這樣確保 memory 中 top-k 個相似 entries 的組合 ，又用 softmax 對所有 entry 進行更新 作者將其應用於框架中的 memory queue 和 memory matrix。\n# 3.3. Formulating Anomaly Detection as Inpainting # Motivation Image in-painting(影像修復) 最初是為了恢復具有鄰近上下文的損壞的影像區域而被提出。因此，作者將異常射線照相圖案修復到健康射線照相圖案中來實現異常檢測。\n修復的影像區域通常被認為與 boundary artifacts(邊界偽影)相關，當進行像素級修復任務時，不良偽影會導致大量誤報。因此作者在特徵層級實現了修復任務。潛在特徵對像素級雜訊、旋轉和平移具有更好的不變性，因此更適合後續的異常檢測。\n# In-painting block 作者將 Memory Queue 整合到一個新穎的 in-painting block 中以執行特徵空間修復，修復三步驟如圖。\n【Fig(a)】 : 在 queue 中增強 $w×h$ 個非重疊 patch 特徵 $F_{{(1,1),\u0026hellip;,(w, h)}}$ 到最相似的 normal patterns $N_{{(1,1),\u0026hellip;,(w, h)}}$。 【Fig(b)】 : 由於 $N$ 是由從先前的訓練資料中提取的特徵組成，因此 $N$ 不受當前輸入影像的影響。為了回顧輸入影像的特徵，我們使用 transformer block 來聚合 patch 特徵 $F$ 及其增強特徵 $N$。 對於每個 patch $F_{i,j}$，其相鄰的八個增強區塊 $N_{(i-1,j-1),\u0026hellip;,(i+1,j+1)}$ 被用做細化(refine) $F_{i,j}$ 的條件。 Transformer block 的 query token 被展平為 $F_{i,j}∈R^{1×*}$ ； 而 key/value tokens 則展平為 $N_{(i-1,j-1),\u0026hellip;,(i+1,j+1)}∈R^{8×*}$。\n在 in-painting block 的開始和結束處，作者用了一對額外的 point-wise convolutions（1×1 卷積核）。\n# Masked shortcut 作者在 in-painting block 中使用，以此更好地聚合特徵並簡化最佳化。\n作者實證表明，direct residual connection（直接殘差連接）會降低 in-painting block 的有效性，因此在訓練期間利用　random binary mask　來門控快捷特徵【Fig(b)】。給定輸入 patch 特徵 $F$，in-painting block 的輸出透過以下方式獲得：\n$F\u0026rsquo;=(1-δ)⋅F+δ⋅inpaint(F)$\ninpaint(·): 設計的 in-painting block δ ~ Bernoulli(ρ): 具有閘控機率(gating probability)的二元變數 在每個訓練步獲得 $F\u0026rsquo;$ 後，原始的 $F$ 被複製來更新記憶體【Fig(c)】。在推理過程中，完全 shortcut，以便 $F\u0026rsquo; = inpaint(F)$ 進行確定性預測。\n# 3.4 Anomaly Discrimination 鑑別器評估重建影像，若判定不現實表示發生異常。 因生成器以正常影像上進行訓練，memory queue 僅儲存 normal pattern。因此在推理中，abnormal pattern 重建的影像預計會顯得不真實。 小結 in-painting block 專注在將任何 patch(正常或異常) 特徵增強為類似的“正常”特徵。 student generator 據“正常”特徵重建“正常”圖像 teacher generator 用於防止學生無論輸入如何都產生相同的圖像。 沒異常的話，input 和 teacher generator 重建的圖像在語意上應該相差很小。因此，我們委託優化的鑑別器網路來感知異常警報，異常分數計算如下。\n$A=ϕ(\\frac{D(G_{s}(E(I)))-μ}{σ})$\nE: encoder $G_t$: teacher generator $G_s$: student generator D: discriminator A: 異常分數 ϕ(⋅): sigmoid function μ 和 σ 是根據 training samples 算出的異常分數的平均值和標準差 # 3.5. Loss Function 共有五種，自己看論文 :D\n# 4. Experiments # 4.1. New Benchmark # DigitAnatomy(數位解剖學) 作者創建了一個合成資料集，圖片中有數字 1~9，數字順序正確被視為「正常」；否則異常。異常包括缺失、亂序、翻轉和 zero digit。\n該資料集對於放射線成像尤其有利\n模擬其兩個獨特屬性，即空間相關性和一致形狀。 無需生物專業知識，更容易進行問題調試。 容易獲得模擬異常的 ground truth。 # 4.2. Public Benchmarks ZhangLab Chest X-ray\n包含健康和肺炎（作為異常）影像 訓練集 1349 張正常 3883 張異常 測試集 234 張正常 390 張異常 作者從訓練集中隨機選 200 張影像（100 張正常和 100 張異常影像）作為超參數調整的驗證集。 所有影像的大小調整為 128x128。 Stanford CheXpert\n對 front-view PA 影像進行評估，共有 12 種異常 訓練集 5249 張正常 23671 張異常 使用和 ZhangLab 相同的超參數 測試集 用訓練集的 250 張正常和 250 張異常進行測試 # 4.3. Baselines and Metrics 13 個 baseline\n經典 UAD(unsupervised anomaly detection) 方法 Auto-Encoder, VAE 醫學影像的 SOTA Ganomaly、f-AnoGAN、IF、SALAD、 最新的 UAD 方法 emAE、CutPaste、M-KD、PANDA、PaDiM、IGD 標準指標評\nReceiver Operating Characteristic (ROC) curve ROC 曲線下面積 (AUC)、 準確度 (Acc) F1 分數 (F1) 除非有特別註明，不然都是從頭獨立訓練模型至少三次\n# 5. Results # 5.1. Interpreting SQUID on DigitAnatomy 承前面數字圖可見，SQUID 重建的圖像比其他方法更有意義和指示性。這主要歸因於 Space-aware memory，由此產生的字典與獨特的模式及其空間訊息相關聯。\n一旦出現異常（例如丟失數字），in-painting block 將透過從字典中組裝前 top-k 個最相似的 pattern 來將異常特徵增強到其正常對應特徵。\nGAN 傾向於重建訓練樣本平均得到的影像。MemAE 由於其 memory matrix 而表現相對較好，但對於缺失數字的異常效果不佳，在極端異常攻擊上完全失敗。\n# 5.2. Benchmarking SQUID on Chest Radiography 圖中視覺化了 SQUID 在正常和異常影像上的重建結果。正常情況下，SQUID可以很容易地在 Memory Queue 中找到相似的匹配，順利實現重建；對於異常情況，將偽造的正常 pattern 強加到異常特徵中就會產生矛盾。這樣，生成的圖像將與輸入顯著不同，然後由鑑別器捕獲。\n# Limitation 目前形式的 SQUID 無法在像素層級精確定位異常。 因為 SQUID 是一種無監督方法，需要對正常/異常影像進行零手動註釋。 那些像素級別的異常檢測會遭遇放大雜訊的影響，但是由於 SQUID 是在特徵層級進行的，比像素級別更加 robust。 # 5.3. Ablating(消融) Key Properties in SQUID # Component study # Hyper-parameter robustness # Disease-free training requirement? 用於醫學異常檢測的無監督方法並不常見，因為所謂的 UAD 方法不是「無監督的」—— 它們必須僅在無疾病影像上進行訓練。\n在實踐中，要獲得健康圖片需要人工標註。\n在訓練集中考慮 disease-free 從 100% - 50% 的情況，把 SQUID 的 robust 和另外三個 baseline 進行比較。\n從圖中可知，SQUID 的 memory queue 可以自動忽略來自少數的 anatomical patterns，容忍高達 50% 的疾病/健康訓練比率。\n# Conclusion 提出 SQUID，用於從放射線影像中進行無監督異常檢測 SQUID 可以將根深蒂固的解剖結構分類為 recurrent pattern(固定重複出現的 pattern) 在推理中，SQUID可以準確地識別異常情況 SQUID 在ZhangLab 資料集上優於主流方法超過 5 點 AUC，在史丹佛 CheXpert 資料集上優於主流方法 10 點 AUC 合成了 DigitAnatomy 資料集，以類似於放射線照相影像中胸部解剖結構的關鍵屬性 ","date":"2024-04-10T03:30:04+08:00","permalink":"https://charlieUWUuwu.github.io/p/squid/","title":"SQUID"}]