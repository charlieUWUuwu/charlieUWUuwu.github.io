[{"content":" # 說明 就像 ChatGPT 那樣一個字一個字跑出來。這個方法是在 LLaMA-Factory 看到的。 完整 code 可以在這個專案裡面找\n# 用 HuggingFace 模型做流式輸出 HuggingFace 的 v4.30.1 加入了兩個流式輸出的接口\nTextStreamer: 能在 stdout 中流式輸出结果 TextIteratorStreamer：能在自定義 loop 中進行操作 # TextStreamer 1# 來自 https://huggingface.co/docs/transformers/main/generation_strategies 2 3from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer 4 5tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) 6model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) 7inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) 8streamer = TextStreamer(tok) 9 10# Despite returning the usual output, the streamer will also print the generated text to stdout. 11_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20) # TextIteratorStreamer 1# 來自 https://huggingface.co/docs/transformers/main/generation_strategies 2 3from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer 4from threading import Thread 5 6tok = AutoTokenizer.from_pretrained(\u0026#34;gpt2\u0026#34;) 7model = AutoModelForCausalLM.from_pretrained(\u0026#34;gpt2\u0026#34;) 8inputs = tok([\u0026#34;An increasing sequence: one,\u0026#34;], return_tensors=\u0026#34;pt\u0026#34;) 9streamer = TextIteratorStreamer(tok) 10 11# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way. 12generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20) 13thread = Thread(target=model.generate, kwargs=generation_kwargs) 14thread.start() 15generated_text = \u0026#34;\u0026#34; 16for new_text in streamer: 17 generated_text += new_text 18generated_text # 用 ChatGLM 示範 參考 [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 中的「ChatGLM流式回覆Demo」部分\n# 讓 OpenAI 模型也能流式輸出 參考 Huggingface 官方實現的 BaseStreamer 寫出來的。\n# 設定 OpenAI 模型 streaming設置為True，可以不用等一句話完全生完後才得到回覆 StreamingStdOutCallbackHandler 是 Langchain 的一個 callback 處理器，會在語言模型模型生成新 token 時被觸發(on_llm_new_token())，並透過sys.stdout.write(token)與sys.stdout.flush()來確保輸出即時可見。 說明可見這個部落格 1from langchain_openai import ChatOpenAI 2from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler 3 4model = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0) # 撰寫 ChatgptStreamer 主要是透過 Python 的 Iterator 來實現的。\n_on_finalized_text() : 將收到的 text 存到 queue，直到收到 stream 結束訊號(stream_end==True)， model_generate() : 接收語言模型並調用.stream()來獲得流式輸出，最後後塞入stop_signal表輸出結束。 1from queue import Queue 2 3class ChatgptStreamer: 4 def __init__(self, timeout: float=3): 5 self.text_queue = Queue() 6 self.stop_signal = None 7 self.timeout = timeout 8 9 def _on_finalized_text(self, text: str, stream_end: bool = False): 10 \u0026#34;\u0026#34;\u0026#34;Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\u0026#34;\u0026#34;\u0026#34; 11 self.text_queue.put(text, timeout=self.timeout) 12 if stream_end: 13 self.text_queue.put(self.stop_signal, timeout=self.timeout) 14 15 def model_generate(self, model, system, query): 16 for chunk in model.stream(f\u0026#34;請你根據以下參考資料回答問題。\\n相關資料:{system}\\n問題:{query}\u0026#34;): 17 self._on_finalized_text(chunk.content) 18 self._on_finalized_text(self.stop_signal, True) 19 20 def __iter__(self): 21 return self 22 23 def __next__(self): 24 value = self.text_queue.get(timeout=self.timeout) 25 if value == self.stop_signal: 26 raise StopIteration() 27 else: 28 return value # Streamer+Thread 進行生成 1@torch.inference_mode() 2def stream_chat( 3 self, 4 query: str, 5 history: Optional[List[Tuple[str, str]]] = None, 6 system: Optional[str] = None, 7 **input_kwargs 8) -\u0026gt; Generator[str, None, None]: 9 gen_kwargs, _ = self._process_args(query, history, system, **input_kwargs) 10 if gen_kwargs == {}: 11 # 使用 ChatOpenAI 12 streamer = ChatgptStreamer(timeout=3) 13 thread = Thread(target=streamer.model_generate, args=(self.model, system, query)) 14 else: 15 # 使用 Huggingface 模型 16 streamer = TextIteratorStreamer(self.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True) 17 gen_kwargs[\u0026#34;streamer\u0026#34;] = streamer 18 19 thread = Thread(target=self.model.generate, kwargs=gen_kwargs) 20 21 thread.start() 22 yield from streamer # 使用 1def predict( 2 self, 3 chatbot: List[Tuple[str, str]], 4 query: str, 5 history: List[Tuple[str, str]], 6) -\u0026gt; Generator[Tuple[List[Tuple[str, str]], List[Tuple[str, str]]], None, None]: 7 chatbot.append([query, \u0026#34;\u0026#34;]) 8 print(\u0026#34;chatbot : \u0026#34;, chatbot) 9 response = \u0026#34;\u0026#34; 10 # 從 VDB 獲取資料 11 system = \u0026#34;\u0026#34; 12 docs = self.vectordb_manager.query(query, n_results=5) # 回傳最相關的 5 筆相關資料 13 for doc in docs: 14 system += doc.page_content 15 print(\u0026#34;系統提示詞 : \u0026#34;, system) 16 17 for new_text in self.stream_chat( 18 query, None, system 19 ): 20 response += new_text 21 chatbot[-1] = [query, self.postprocess(response)] 22 yield chatbot, history # refs : [AI]如何让语言模型LLMs流式输出：HuggingFace Transformers实现 HuggingFace 官方文件 ","date":"2024-04-10T03:30:04+08:00","permalink":"https://charlieUWUuwu.github.io/p/chat_streamer/","title":"Chat_streamer"}]